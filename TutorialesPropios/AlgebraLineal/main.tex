
%
%
%     File Name :
%
%       Purpose :
%
% Creation Date :
%
% Last Modified : Fri 22 Feb 2013 05:23:46 PM ART
%
%    Created By :  Ezequiel Castillo
%
%

\documentclass[a4paper,12pt]{article}

\include{mypreamble}

\begin{document}

%------------------------------------------
%------------------------------------------
%------------------------------------------
\section{Matrices}


%------------------------------------------
%------------------------------------------
\subsection{Sistemas de ecuaciones lineales y matrices}

%------------------------------------------
\subsubsection{Ecuaciones lineales}

\begin{concept}[i]
  Todo sistema de ecuaciones lineales no tiene soluciones, tiene exactamente
  una solución o tiene una infinidad de soluciones.
\end{concept}

Las \textbf{\emph{operaciones elementales}} en las filas son las siguientes:

\begin{concept}
  \begin{enumerate}
    \item Multiplicar una fila por una constante diferente de cero.
    \item Intercambiar dos filas.
    \item Sumar un múltiplo de una fila a otra fila.
  \end{enumerate}
\end{concept}

%------------------------------------------
\subsubsection{Sistemas lineales homogéneos}
Un sistema de ecuaciones lineales es \textbf{\emph{homogéneo}} si todos los
términos constantes son cero; es decir el sistema es de la forma:
\begin{align*}
  \begin{matrix}
    a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n &=& 0     \\
    a_{21}x_1 &+& a_{22}x_2 &+& \cdots &+& a_{2n}x_n &=& 0     \\
    \vdots    & &\vdots     & &        & &\vdots     & & \vdots\\
    a_{m1}x_1 &+& a_{m2}x_2 &+& \cdots &+& a_{mn}x_n &=& 0
  \end{matrix}
\end{align*}

Todo sistema de ecuaciones lineales homogéneo es consistente, ya que siempre
existe la \textbf{\textit{solución trivial}} (es decir, $x_1=1$, $x_2=1$,
\ldots, $x_n=1=0$).
Debido a que un sistema lineal homogéneo siempre tiene la solución trivial,
entonces para sus soluciones sólo hay dos posibilidades.

\begin{concept}
  \begin{itemize}
    \item El sistema tiene sólo la solución trivial.
    \item El sistema tiene infinidad de soluciones además de la solución
      trivial.
  \end{itemize}
\end{concept}

\begin{theorem}
  Un sistema de ecuaciones lineales homogéneo con más incógnitas que
  ecuaciones tiene infinidad de soluciones
  \label{theo:1}
\end{theorem}

%------------------------------------------
\subsubsection{Matrices y operaciones con matrices}

\begin{concept}[i]
  Una \textbf{matriz} es un arreglo rectangular de números. Los números en el arreglo
  se denominan \textbf{elementos} de la matriz.
\end{concept}

Una matriz general $m\times n$ puede expresarse como:
\begin{align*}
  A = \begin{bmatrix}
    a_{11}& a_{12}& \cdots& a_{1n}\\
    a_{21}& a_{22}& \cdots& a_{2n}\\
    \vdots&\vdots &       &\vdots \\
    a_{m1}& a_{m2}& \cdots& a_{mn}
  \end{bmatrix}
  = \left[ a_{ij} \right]_{m\times n} = \left[ a_{ij} \right]
\end{align*}

A continuación definiremos en que consiste una matriz que se encuentra en su
\bi{forma escalonada reducida}:
\begin{concept}[i]
  \begin{enumerate}
    \item Si una fila no consta completamente de ceros, entonces el primer
      número diferente de cero en la fila es un 1. (Que se denomina
      \textbf{1 principal}).
    \item Si hay filas que constan completamente de ceros, se agrupan en
      la parte inferior de la matriz.
    \item En dos filas consecutivas cualesquiera que no consten completamente
      de ceros, el 1 principal de la fila inferior aparece más a la derecha
      que el 1 principal de la fila superior.
    \item Cada columna que contenga un 1 principal tiene cero en todas las
      demás posiciones.
  \end{enumerate}
\end{concept}

%------------------------------------------
\subsubsection{Operaciones con matrices}

\begin{concept}[i]
  Dos matrices son \textbf{iguales} si tienen el mismo tamaño y sus elementos
  correspondientes son iguales.
\end{concept}

\begin{concept}[i]
  Si $A$ y $B$ son matrices del mismo tamaño, entonces la \textbf{suma}
  $A+B$ es la matriz obtenida al sumar los elementos de $B$ con los elementos
  correspondientes de $A$. No es posible sumar o restar matrices de tamaños
  diferentes.
\end{concept}
En notación matricial:
  \begin{align*}
    (A+B)_{ij} = (A)_{ij} + (B)_{ij} = a_{ij} + b_{ij} \\
    (A-B)_{ij} = (A)_{ij} - (B)_{ij} = a_{ij} - b_{ij}
  \end{align*}

\begin{concept}[i]
  Si $A$ es cualquier matriz y $c$ es cualquier escalar, entonces el
  \textbf{producto} $cA$ es la matriz obtenida al multiplicar cada elemento de
  $A$ por $c$.
\end{concept}
En notación matricial:
\begin{equation*}
  (cA)_{ij} = c(A)_{ij} = ca_{ij}
\end{equation*}

\begin{concept}
  Si $A$ es una matriz $m\times r$ y $B$ es una matriz $r\times n$, entonces
  el \textbf{producto} $AB$ es la matriz $m\times n$ cuyos elementos se
  determinan como sigue. Para encontrar el elemento en la fila $i$ y en la
  columna $j$ de $AB$, considerar sólo la fila $i$ de la Matriz $A$ y la
  columna $j$ de la matriz $B$. Multiplicar entre si los elementos
  correspondientes del renglón y de la columna mencionados y luego sumar los
  productos resultantes.
\end{concept}
En notación matricial:
\begin{align*}
  \left[ (AB)_{ij} \right]_{m\times n} = \sum_{k=1}^r A_{ik}B_{kj}
\end{align*}
La matriz resultante será de $m\times n$.

%------------------------------------------
\subsubsection{Multiplicación de matrices por columnas y por renglones}

\begin{align*}
  j^{th} \textrm{ matriz columna de } AB &= A \left[ j^{th} \textrm{matriz columa de} B
  \right] \\
  i^{th}\textrm{ matriz fila de } AB &= \left[ j^{th} \textrm{ matriz columa de } A
  \right] B
\end{align*}

Si $\textbf{a}_1$, $\textbf{a}_2$, \ldots, $\textbf{a}_m$ denotan las matrices
fila de $A$ y $\textbf{b}_1$, $\textbf{b}_2$, \ldots, $\textbf{b}_n$ denotan
las matrices columna de $B$, entonces por lo establecido recién se concluye
que:

\begin{align*}
  AB =
  A\begin{bmatrix}\textbf{b}_1&\textbf{b}_2&\cdots&\textbf{b}_n \end{bmatrix}
    =
    \begin{bmatrix}A\textbf{b}_1&A\textbf{b}_2&\cdots&A\textbf{b}_n\end{bmatrix}
\end{align*}

\begin{align*}
  AB = \begin{bmatrix}
    \textbf{a}_1 \\
    \textbf{a}_2 \\
    \vdots       \\
    \textbf{a}_m
  \end{bmatrix} B = \begin{bmatrix}
    \textbf{a}_1B \\
    \textbf{a}_2B \\
    \vdots        \\
    \textbf{a}_mB
  \end{bmatrix}
\end{align*}

%------------------------------------------
\subsubsection{Productos de matrices como combinaciones lineales}

Sean:

\begin{align*}
  A = \begin{bmatrix}
    a_{11}& a_{12}& \cdots& a_{1n}\\
    a_{21}& a_{22}& \cdots& a_{2n}\\
    \vdots&\vdots &       &\vdots \\
    a_{m1}& a_{m2}& \cdots& a_{mn}
  \end{bmatrix} &
  & \textbf{x} = \begin{bmatrix}
    x_1    \\
    x_2    \\
    \vdots \\
    x_m
  \end{bmatrix} &
  & \textbf{b} = \begin{bmatrix}
    b_1    \\
    b_2    \\
    \vdots \\
    b_m
  \end{bmatrix} &
\end{align*}

Mediante esta elección es posible expresar al sistema de ecuaciones:
\begin{align*}
  \begin{matrix}
    a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n &=& b_1     \\
    a_{21}x_1 &+& a_{22}x_2 &+& \cdots &+& a_{2n}x_n &=& b_2     \\
    \vdots    & &\vdots     & &        & &\vdots     & & \vdots\\
    a_{m1}x_1 &+& a_{m2}x_2 &+& \cdots &+& a_{mn}x_n &=& b_m
  \end{matrix}
\end{align*}
como:
\begin{align*}
  A\textbf{x}=\textbf{b}
\end{align*}
La matriz $A$ se denomina \textbf{\emph{matriz de coeficientes}} del sistema.


%------------------------------------------
\subsubsection{Transpuesta de una matriz}

\begin{concept}[i]
  Si $A$ es cualquier matriz $m\times n$, entonces la \textbf{transpuesta de
    $A$}, denotada por $A^T$, se define como la matriz $n\times m$ que se
    obtiene al intercambiar las filas y las columnas de $A$, es decir, la
    primera columna de $A^T$ es la primer fila de $A$, la segunda columna de
    $A^T$ es la segunda fila de $A$, y así sucesivamente.
\end{concept}
En notación matricial:
\begin{align*}
  \left( A^T \right)_{ij} = (A)_{ji}
\end{align*}

%------------------------------------------
\subsubsection{Traza de una matriz}

\begin{concept}[i]
  Si $A$ es una matriz cuadrada, entonces la \textbf{\emph{traza de A}},
  denotada por $\tr (A)$, se define como la suma de la diagonal principal de
  $A$. La traza de $A$ no está definida si $A$ no es una matriz cuadrada.
\end{concept}
En notación matricial:
\begin{align*}
  \tr (A)_{n\times n}=\sum_{i=1}^n a_{ii}
\end{align*}

%------------------------------------------
%------------------------------------------
\subsection{Reglas de la aritmética de matrices}

%------------------------------------------
\subsubsection{Propiedades de las operaciones con matrices}

Muchas de las reglas básicas de la aritmética de los números reales también se
cumplen para matrices, aunque unas cuantas no. Por ejemplo, para números
reales $a$ y $b$ siempre se cumple que $ab=ba$ (\emph{ley conmutativa de la
multiplicación}). Para matrices, sin embargo, $AB$ y $BA$ no necesariamente
son iguales.
\begin{theorem}
  Suponiendo que los tamaños de las matrices son tales que las operaciones
  indicadas se pueden efectuar, entonces son válidas las siguientes reglas de
  aritmética matricial.

  \begin{enumerate}[(a)]
    \item $A+B=B+A$ \hfill \bi{Ley conmutativa de la adición}
    \item $A+(B+C)=(A+B)+C$ \hfill \bi{Ley asociativa de la adición}
    \item $A(BC)=(AB)C$ \hfill \bi{Ley asociativa de la multiplicación}
    \item $A(B+C)=AB+AC$ \hfill \bi{Ley distributiva por la izquierda}
    \item $(B+C)A=BA+CA$ \hfill \bi{Ley distributiva por la derecha}
    \item $A(B-C)=AB-AC$
    \item $(B-C)A=BA-CA$
    \item $a(B+C)=aB+aC$
    \item $a(B-C)=aB-aC$
    \item $(a+b)C=aC+bC$
    \item $(a-b)C=aC-bC$
    \item $a(bC)=(ab)C$
    \item $a(BC)=(aB)C=B(aC)$
  \end{enumerate}
  \label{theo:aritmat}
\end{theorem}

\demo Probaremos el inciso \emph{(d)}. Para ello es necesario probar que
$A(B+C)$ y $AB+AC$ son del mismo tamaño y que los elementos correspondientes
son iguales. Para formar $A(B+C)$, $B$ y $C$ deben ser del mismo tamaño, por
ejemplo, $n\times m$. Entonces $A$ debe tener el mismo número de columnas para
que la multiplicación pueda llevarse a cabo, digamos por ejemplo, $r\times n$
de modo que $A(B+C)$ tendrá dimensiones $r\times m$. Veamos ahora la otra
igualdad. Con estas definiciones para $A$, $B$ y $C$ se cumple que tanto
$AB$ como $AC$ tienen dimensiones de $r\times n$. Por lo tanto $A(B+C)$ y
$AB+AC$ son del mismo tamaño.
Queda entonces probar que los elementos correspondientes de $A(B+C)$ y $AB+AC$
son iguales, es decir que:
\begin{align*}
  [A(B+C)]_{ij}=[AB+AC]_{ij}
\end{align*}
para todos los valores de $i$ y $j$. Por las definiciones de adición y de
multiplicación de matrices se tiene:
\begin{align*}
  \left[ A(B+C)_{ij}
  \right]=&a_{i1}(b_{1j}+c_{1j})+a_{i2}(b_{2j}+c_{2j})+\cdots+a_{im}(b_{mj}+c_{mj}) \\
         =&a_{i1}b_{1j}+a_{i1}c_{1j}+a_{i2}b_{2j}+a_{i2}c_{2j}+\cdots+a_{im}b_{mj}+a_{im}c_{mj}
\end{align*}
Pero:
\begin{align*}
  a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{im}b_{mj}=[AB]_{ij} \\
  a_{i1}c_{1j}+a_{i2}c_{2j}+\cdots+a_{im}c_{mj}=[AC]_{ij}
\end{align*}
Por lo tanto:
\begin{align*}
  [A(B+C)]_{ij}=&[AB]_{ij}+[AC]_{ij} \\
               =&[AB+AC]_{ij}
\end{align*}
Con esto queda demostrado que los elementos correspondientes de $A(B+C)$ y
$AB+AC$ son iguales.

La demostración del inciso \emph{c} es más complicada.

%------------------------------------------
\subsubsection{Matrices cero}

\begin{concept}
  Una matriz que tiene sus elementos iguales a cero se denomina \bi{matriz
  cero}.
\end{concept}

Como ya se sabe que algunas de las reglas de la aritmética para los números reales
no se cumplen en la aritmética matricial, es temerario asumir que todas las
propiedades del número real cero se cumplen para las matrices cero. En la
aritmética de números reales se cumple que:
\begin{itemize}
  \item Si $ab=ac$ y $a\ne 0$, entonces $b=c$ (\emph{ley de cancelación}).
  \item Si $ad=0$ entonces por lo menos uno de los factores del miembro
    izquierdo es cero.
\end{itemize}
En general los resultados correspondientes \textbf{no} son ciertos en aritmética
matricial.

A pesar de lo dicho anteriormente, existen varias propiedades conocidas de
número real cero que \emph{se cumplen} en las matrices cero:
\begin{theorem}
  Suponiendo que los tamaños de las matrices son tales que las operaciones
  indicadas se pueden efectuar, entonces son válidas las siguientes reglas de
  aritmética matricial.
  \begin{enumerate}[(a)]
    \item $A+0=0+A=A$
    \item $A-A=0$
    \item $0-A=-A$
    \item $A0=0$; $0A=0$
  \end{enumerate}
  \label{theo:cero}
\end{theorem}

%------------------------------------------
\subsubsection{Matrices identidad}

\begin{concept}
  Una matriz \textit{cuadrada} que tiene unos en la diagonal principal y ceros
  fuera de ésta se denomina \bi{matriz identidad}.
\end{concept}

En aritmética matricial la matriz identidad juega un papel bastante semejante
al que desempeña el número 1 en las relaciones numéricas $a\cdot1=1\cdot a=a$:
\begin{align*}
  AI=A \qquad \mbox{y} \qquad IA=A
\end{align*}

Como se muestra en el teorema siguiente, las matrices identidad surgen
naturalmente en el estudio de formas escalonadas reducidas de matrices
\emph{cuadradas}.

\begin{theorem}
  Si $R$ es la forma escalonada reducida de una matriz $A$ de $n\times n$,
  entonces $R$ tiene un renglón de ceros, o bien, $R$ es la matriz identidad
  $I_n$.
\end{theorem}

%------------------------------------------
\subsubsection{Inversa de una matriz}

\begin{concept}
  Si $A$ es una matriz cuadrada y si se puede encontrar una matriz $B$ del
  mismo tamaño tal que $AB=BA=I$, entonces se dice que $A$ es
  \bi{invertible} y $B$ se denomina \bi{inversa} de $A$
\end{concept}

%------------------------------------------
\subsubsection{Propiedades de las inversas}

\begin{theorem}
  Si $B$ y $C$ son, ambas, inversas de la matriz $A$, entonces $B=C$
  \label{theo:propinv}
\end{theorem}

\demo Ya que $B$ es inversa de $A$ se tiene que:
\begin{align*}
     BA=&I && \mbox{(\emph{multiplicando por derecha} por $C$ ambos miembros)}\\
  (BA)C=&IC&& \mbox{(ley asociativa de la multiplicación)}\\
  B(CA)=&C && \\
     BI=&C && \mbox{(suposición inicial)} \\
      B=&C  
\end{align*}

Otra propiedad de las inversas:
\begin{align*}
  AA^{-1}=I \qquad \mbox{y} \qquad A^{-1}A=I
\end{align*}

\begin{theorem}
  Si $A$ y $B$ son matrices invertibles del mismo tamaño, entonces:
  \begin{enumerate}[(a)]
    \item $AB$ es invertible
    \item $(AB)^{-1}=B^{-1}A^{-1}$
  \end{enumerate}
  \label{theo:compinv}
\end{theorem}

\demo Si se puede demostrar que:
\begin{align*}
  (AB)(B^{-1}A^{-1})=(B^{-1}A^{-1})(AB)=I
\end{align*}
entonces se habrá demostrado simultáneamente que la matriz $AB$ es invertible
y que $(AB)^{-1}=B^{-1}A^{-1}$.
Siguiendo el siguiente razonamiento:
\begin{align*}
  (AB)(B^{-1}A^{-1})=&(B^{-1}A^{-1})(AB) \\
  A(BB^{-1})A^{-1}=&B^{-1}(A^{-1}A)B \\
  AIA^{-1}=&B^{-1}IB \\
  AA^{-1}=&B^{-1}B \\
  I=&I
\end{align*}

Aunque la siguiente declaración no se demostrará, este último concepto se
puede extender para incluir tres o más factores.
\begin{concept}[i]
  Un producto de cualquier número de matrices invertibles es invertible, y la
  inversa del producto es el producto de las inversas en orden invertido.
\end{concept}

%------------------------------------------
\subsubsection{Potencias de una matriz}
\begin{concept}
  Si $A$ es una matriz cuadrada, entonces las potencias enteras no negativas
  se definen como:
  \begin{align*}
    A^0=I \qquad \mbox{y} \qquad A^n=\prod_i^nA \quad\mbox{con $n>0$}
  \end{align*}
  A su vez, si $A$ es invertible, entonces las potencias negativas de
  $A$ se definen como:
  \begin{align*}
    A^{-n} = \left( A^{-1} \right)^n = \prod_i^nA^{-1} \quad\mbox{con $n>0$}
  \end{align*}
\end{concept}

\begin{theorem}
  Si $A$ es una matriz cuadrada y $r$ y $s$ son enteros, entonces:
  \begin{align*}
    A^rA^s=A^{r+s} \qquad \mbox{y} \qquad (A^r)^s=A^{rs}
  \end{align*}
  \label{theo:expmatr}
\end{theorem}

\begin{theorem}
  Si $A$ es una matriz invertible, entonces:
  \begin{enumerate}[(a)]
    \item $A^{-1}$ es invertible y $(A^{-1})^{-1}=A$.
    \item $A^n$ es invertible y $(A^n)^{-1}=(A^{-1})^n$ para $n=0,1,2,$\ldots
    \item Para cualquier escalar $k$ diferente de cero, la matriz $kA$ es
      invertible y $(kA)^{-1}=\tfrac{1}{k}A^{-1}$.
  \end{enumerate}
  \label{theo:expmatr2}
\end{theorem}

\demo
\begin{enumerate}[(a)]
  \item Como $AA^{-1}=A^{-1}A=I$, la matriz $A^{-1}$ es invertible y
    $(A^{-1})^{-1}=A$.
  \item De la extensión del teorema \ref{theo:compinv} (no demostrado aquí)
    podemos decir que debido a que $A$ es invertible entonces $A^n$ es
    invertible y su inversa es el producto de las inversas en orden invertido.
    Esto es:
    \begin{align*}
      \left( A^n \right)^{-1} &= \left( A^{-1} \right)^n \\
      &= A^{-n}
    \end{align*}
  \item Si $k$ es cualquier escalar diferente de cero, entonces por los
    resultados \emph{(l)} y \emph{(m)} del teorema \ref{theo:aritmat} es
    posible escribir:
    \begin{align*}
      (kA)\left( \frac{1}{k}A^{-1} \right) &= \frac{1}{k}(kA)A^{-1} \\
                                            &=\left( \frac{1}{k}k \right)AA^{-1} \\
                                            &= 1I \\
                                            &= I
    \end{align*}
\end{enumerate}


%------------------------------------------
\subsubsection{Propiedades de la transpuesta}

\begin{theorem}
  Si los tamaños de las matrices son tales que se pueden efectuar las
  operaciones planteadas, entonces:
  \begin{enumerate}[(a)]
    \item $\left( (A)^T \right) ^T=A$
    \item $(A+B)^T=A^T+B^T$ y $(A-B)^T=A^T-B^T$
    \item $(kA)^T=kA^T$, donde $k$ es cualquier escalar
    \item $(AB)^T=B^TA^T$
  \end{enumerate}
  \label{theo:operactrans}
\end{theorem}

\demo Al considerar que al transponer una matriz se intercambian sus filas por
sus columnas, los incisos \emph{(a)}, \emph{(b)} y \emph{(c)} deben ser
evidentes. Para demostrar el inciso \emph{(d)} supongamos:
\begin{align*}
  A=\left[ a_{ij} \right]_{m\times r} \qquad \mbox{y} \qquad B=\left[ b_{ij} \right]_{r\times n}
\end{align*}
A continuación demostraremos que $(AB)^T$ y $B^TA^T$ son del mismo tamaño. El
producto $AB$ tendrá un tamaño de $m\times n$. Ahora bien, la transpuesta del
producto $(AB)^T$ tendrá dimensiones $n\times m$. Por otro lado:
\begin{align*}
  A^T=\left[ a'_{ij} \right]_{r\times m} \qquad \mbox{y} \qquad B^T=\left[ b'_{ij} \right]_{n\times r}
\end{align*}
De modo que el producto $B^TA^T$ tendrá un tamaño $n\times m$. Por lo que
$(AB)^T$ y $B^TA^T$ tienen ambos el mismo tamaño.
Nos queda demostrar que los elementos correspondientes de $(AB)^T$ y $B^TA^T$
son los mismos, es decir:
\begin{align*}
  \left( (AB)^T \right)_{ij} = \left( B^TA^T \right)_{ij}
\end{align*}
Para encontrar $\left( (AB)^T \right)_{ij}$ primero encontramos el producto
$AB$:
\begin{align*}
  (AB)_{ij}&= a_{i1}b_{1j}+a_{i2}b_{2j}+\ldots+a_{ir}b_{rj} \\
  &= \sum_{k=1}^r a_{ik}b_{kj}
\end{align*}
La transpuesta de este último resultado es:
\begin{align*}
  \left( (AB)^T \right)_{ij} &= (AB)_{ji} \\
  &= \sum_{k=1}^r a_{jk}b_{ki}
\end{align*}
Notar el intercambio de subíndices con respecto al último resultado.
Analicemos ahora el otro miembro de la igualdad $\left( B^TA^T \right)_{ij}$.
\begin{align*}
  \left( B^TA^T \right)_{ij} &= \sum_{k=1}^r b'_{ik}a'_{kj} \\
  &= \sum_{k=1}^r b_{ki}a_{jk} \\
  &= \sum_{k=1}^r a_{jk}b_{ki}
\end{align*}
Con esto queda demostrado que $(AB)^T$ y $B^TA^T$ son iguales.

Aunque no se demostrará el siguiente hecho, el inciso \emph{(d)} del último
teorema se puede extender para incluir tres o más factores:
\begin{concept}[i]
  La transpuesta de un producto de cualquier número de matrices es igual al
  producto de sus transpuestas en orden invertido.
\end{concept}

%------------------------------------------
\subsubsection{Invertibilidad de una transpuesta}

\begin{theorem}
  Si $A$ es una matriz invertible, entonces $A^T$ también es invertible y:
  \begin{align}
    \left( A^T \right)^{-1} = \left( A^{-1} \right)^T
    \label{eqn:invtrans}
  \end{align}
  \label{theo:invtrans}
\end{theorem}

\demo Se puede probar la invertibilidad de $A^T$ y obtener \eqref{eqn:invtrans}
al demostrar que:
\begin{align*}
  A^T\left( A^{-1} \right)^T = \left( A^{-1} \right)^T A^T = I
\end{align*}
Por el inciso \emph{(d)} del teorema \ref{theo:operactrans}:
\begin{align*}
  \left( A^T\left( A^{-1} \right)^T \right)^T &= \left( \left( A^{-1} \right)^T
  A^T \right)^T \\
  \left( \left( A^{-1} \right)^T \right)^T \left( A^T \right)^T &= \left( A^T
  \right)^T \left( \left( A^{-1} \right)^T \right)^T \\
  A^{-1}A &= AA^{-1} \\
  I &= I
\end{align*}


%------------------------------------------
\subsection{Matrices elementales y un método para determinar $A^{-1}$}

%------------------------------------------
\subsubsection{Matrices elementales}

\begin{concept}
  Una matriz $n\times n$ se denomina \bi{matriz elemental} si se puede obtener
  a partir de la matriz identidad $I_n$ al efectuar una sola operación
  elemental en las filas.
\end{concept}

\begin{theorem}
  Toda matriz elemental es invertible, y la inversa es también una matriz
  elemental
  \label{theo:elematr}
\end{theorem}

\demo Si $E$ es una matriz elemental, entonces $E$ se obtiene al efectuar
algunas operaciones en las filas de $I$. Sea $E_0$ la matriz que se obtiene
cuando la inversa de esta operación se efectúa en I. Entonces usando el hecho
de que las operaciones inversas en las filas cancelan mutuamente su efecto, se
concluye que:
\begin{align*}
  E_0E=I \qquad \mbox{y} \qquad EE_0=I
\end{align*}

El siguiente teorema establece algunas relaciones fundamentales entre
invertibilidad, sistemas lineales homogéneos, formas escalonadas reducidas y
matrices elementales.

\begin{theorem}
  Si $A$ es una matriz $n\times n$, entonces las siguientes proposiciones son
  equivalentes, es decir, todas son verdaderas o todas son falsas.
  \begin{enumerate}[(a)]
    \item $A$ es invertible.
    \item $A\xvec=\textbf{0}$ sólo tiene la solución trivial.
    \item La forma escalonada reducida de $A$ es $I_n$.
    \item A se puede expresar como un producto de matrices elementales.
  \end{enumerate}
  \label{theo:relacfund}
\end{theorem}

\demo Se demostrará la equivalencia estableciendo la cadena de implicaciones
$a\Rightarrow b\Rightarrow c\Rightarrow d\Rightarrow a$.

$a\Rightarrow b$: Si $A$ es invertible y sea  $\xvec_0$ cualquier solución de
$A\xvec=\textbf{0}$; así, $A\xvec_0=0$. Al multiplicar ambos miembros de la
ecuación por la matriz $A^{-1}$ se obtiene:
\begin{align*}
  A^{-1}(A\xvec_0) &= A^{-1}\textbf{0} \\
  (A^{-1}A)\xvec_0 &= \textbf{0} \\
          I\xvec_0 &= 0 \\
           \xvec_0 &= 0
\end{align*}
Por lo tanto $A\xvec_0=\textbf{0}$ sólo tiene la solución trivial.

$b\Rightarrow c$: Sea $A\xvec=\textbf{0}$. La matriz:
\begin{align*}
  \begin{bmatrix}
    a_{11}& a_{12}& \cdots& a_{1n} \\
    a_{21}& a_{22}& \cdots& a_{2n} \\
    \vdots&\vdots &       &\vdots  \\
    a_{m1}& a_{m2}& \cdots& a_{mn} 
  \end{bmatrix}
\end{align*}
puede llevarse por medio de operaciones elementales en las filas a:
\begin{align*}
  \begin{bmatrix}
    1&   0& \cdots& 0 \\
    0&   1& \cdots& 0 \\
    \vdots&\vdots & \ddots  &\vdots  \\
    0&   0& \cdots& 1 
  \end{bmatrix}
\end{align*}
Por lo tanto la forma escalonada reducida de $A$ es la matriz $I_n$.

$b\Rightarrow c$: Suponer que la forma escalonada reducida de $A$ es
$I_n$ implica que $A$ se puede reducir a $I_n$ mediante una sucesión finita de
operaciones elementales en las filas. Esto es lo mismo que escribir:
\begin{align}
  E_k\cdots E_2E_1A=I_n
  \label{eqn:oem}
\end{align}
Por el teorema \ref{theo:elematr}, las matrices $E_k\cdots E_2E_1A=I_n$ son
invertibles. Al multiplicar por izquierda a ambos miembros de la ecuación
\eqref{eqn:oem} se obtiene:
\begin{align*}
  A=E_1^{-1}E_2^{-1}\cdots E_k^{-1}I_n=E_1^{-1}E_2^{-1}\cdots E_k^{-1}
\end{align*}
Por lo que queda demostrado que $A$ puede expresarse como un producto de
matrices elementales.

$b\Rightarrow c$: Si $A$ es un producto de matrices elementales entonces por
los teoremas \ref{theo:compinv} y \ref{theo:elematr} la matriz es un producto
de matrices invertibles, y por lo tanto es invertible.

%------------------------------------------
\subsubsection{Equivalencia por renglones}
Las matrices que se pueden obtener a partir de otra matriz mediante la
ejecución de una sucesión finita de operaciones elementales en las filas se
denominan \bi{equivalentes por renglones}. Con esta terminología por los
incisos \emph{(a)} y \emph{(c)} del teorema \ref{theo:relacfund} se concluye
que una matriz $A_{n\times n}$ es invertible si y sólo si es equivalentes por
renglones a la matriz identidad $I_{n}$.

%------------------------------------------
\subsubsection{Un método para invertir matrices}

\begin{concept}[i]
  Para determinar la inversa de una matriz invertible $A$, es necesario
  encontrar una sucesión de operaciones elementales en las filas que reduzca a
  $A$ a la matriz identidad y luego efectuar esta misma sucesión de
  operaciones en $I_n$ para obtener $A_{-{1}}$.
\end{concept}


%------------------------------------------
\subsection{Matrices diagonales, triangulares y simétricas}

%------------------------------------------
\subsubsection{Matrices diagonales}

\begin{concept}
  Una matriz cuadrada en la que todos los elementos fuera de la diagonal son
  cero se denomina \bi{matriz diagonal} y puede representarse como:
  \begin{align*}
    D=
  \begin{bmatrix}
    d_1&   0& \cdots& 0 \\
    0&   d_2& \cdots& 0 \\
    \vdots&\vdots & \ddots  &\vdots  \\
    0&   0& \cdots& d_n 
  \end{bmatrix}
  \end{align*}
\end{concept}

Una matriz diagonal es invertible sólo si todos los elementos de su diagonal
son distintos de cero, en este caso la inversa es:
\begin{align*}
  D^{-1}=
\begin{bmatrix}
  1/d_1 &   0   & \cdots  & 0 \\
  0     &  1/d_2& \cdots  & 0 \\
  \vdots&\vdots & \ddots  &\vdots  \\
  0     &   0   & \cdots  & 1/d_n 
\end{bmatrix}
\end{align*}

Por otro lado, las potencias de las matrices diagonales son fáciles de
calcular. Sea $k$ un número entero positivo, entonces:
\begin{align*}
  D^k=
\begin{bmatrix}
  d_1^k &   0   & \cdots  & 0 \\
  0     &  d_2^k& \cdots  & 0 \\
  \vdots&\vdots & \ddots  &\vdots  \\
  0     &   0   & \cdots  & d_n^k 
\end{bmatrix}
\end{align*}

Finalmente para multiplicar una matriz $A$ por izquierda por una matriz
diagonal $D$, es posible multiplicar filas sucesivas de $A$ por los elementos
diagonales sucesivos de $D$; y para multiplicar $A$ por la derecha por
$D$ es posible multiplicar columnas sucesivas de $A$ por los elementos
diagonales sucesivos de $D$.

%------------------------------------------
\subsubsection{Matrices triangulares}

\begin{concept}
  Una matriz \emph{cuadrada} en la que todos los elementos arriba de la
  diagonal principal son cero se denomina \bi{triangular inferior}, y una
  matriz \emph{cuadrada} en la que todos los elementos debajo de la
  diagonal principal son cero se denomina \bi{triangular superior}.
\end{concept}

%------------------------------------------
\subsubsection{Matrices simétricas}

\begin{concept}[i]
  Una matriz cuadrada es \bi{simétrica} si:
  \begin{align*}
    A=A^T
  \end{align*}
\end{concept}

\begin{theorem}
  Si $A$ y $B$ son matrices simétricas del mismo tamaño y si $k$ es cualquier
  escalar, entonces:
  \begin{enumerate}[(a)]
    \item $A^T$ es simétrica.
    \item $A+B$ y $A-B$ son simétricas
    \item $kA$ es simétrica.
  \end{enumerate}
  \label{theo:simetmat}
\end{theorem}

\obse En general no es cierto que el producto de matrices simétricas es
simétrico. Para ver esto, por el inciso \emph{(d)} del teorema
\ref{theo:operactrans}, se tiene:
\begin{align*}
  (AB)^T=B^TA^T=BA
\end{align*}
Como $AB$ y $BA$ suelen ser diferentes, se concluye que en términos generales
$AB$ no es simétrico. Sin embargo, en el caso que $AB=BA$, entonces se dice
que $A$ y $B$ \bi{conmutan}. En resumen:
\begin{concept}[i]
  El producto de dos matrices simétricas es simétrico si y sólo si las
  matrices conmutan.
\end{concept}

\begin{theorem}
  Si $A$ es una matriz simétrica invertible, entonces $A^{-1}$ es simétrica.
  \label{theo:matrsiminv}
\end{theorem}

\demo Supongamos que $A$ es simétrica e invertible. Por el teorema
\ref{theo:invtrans} y el hecho de que $A=A^T$, se tiene:
\begin{align*}
  \left( A^{-1} \right)^T=\left( A^T \right)^{-1}=A^{-1}
\end{align*}
lo que demuestra que $A^{-1}$ es simétrica.

%------------------------------------------
\subsubsection{Matrices de la forma $AA^T$ y $A^TA$}

Los productos de matrices de la forma $AA^T$ y $A^TA$ son siempre simétricos
porque:
\begin{align*}
  \left( AA^T \right)^T = \left( A^T \right)^TA^T = AA^T \quad \mbox{y} \quad
  \left( A^TA \right)^T=A^T\left( A^T \right)^T=A^TA
\end{align*}

\begin{theorem}
  Si $A$ es una matriz invertible, entonces $AA^T$ y $A^TA$ también son
  invertibles.
  \label{theo:invATA}
\end{theorem}

\demo Como $A$ es invertible, entonces por el teorema \ref{theo:invtrans}
también lo es $A^T$. Así, $AA^T$ y $A^TA$ son invertibles, ya que son el
producto de matrices invertibles.


\newpage
%------------------------------------------
%------------------------------------------
%------------------------------------------
\section{Determinantes}
\end{document}
