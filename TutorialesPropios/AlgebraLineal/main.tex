
%
%
%     File Name :
%
%       Purpose :
%
% Creation Date :
%
% Last Modified : Sun 03 Mar 2013 11:56:25 PM ART
%
%    Created By :  Ezequiel Castillo
%
%

\documentclass[a4paper,12pt]{article}

\include{mypreamble}

\begin{document}

\include{config}
%------------------------------------------
%------------------------------------------
%------------------------------------------
\section{Matrices}


%------------------------------------------
%------------------------------------------
\subsection{Sistemas de ecuaciones lineales y matrices}

%------------------------------------------
\subsubsection{Ecuaciones lineales}

\begin{concept}[i]
  Todo sistema de ecuaciones lineales no tiene soluciones, tiene exactamente
  una solución o tiene una infinidad de soluciones.
\end{concept}

Las \textbf{\emph{operaciones elementales}} en las filas son las siguientes:

\begin{concept}
  \begin{enumerate}
    \item Multiplicar una fila por una constante diferente de cero.
    \item Intercambiar dos filas.
    \item Sumar un múltiplo de una fila a otra fila.
  \end{enumerate}
\end{concept}

%------------------------------------------
\subsubsection{Sistemas lineales homogéneos}
Un sistema de ecuaciones lineales es \textbf{\emph{homogéneo}} si todos los
términos constantes son cero; es decir el sistema es de la forma:
\begin{align*}
  \begin{matrix}
    a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n &=& 0     \\
    a_{21}x_1 &+& a_{22}x_2 &+& \cdots &+& a_{2n}x_n &=& 0     \\
    \vdots    & &\vdots     & &        & &\vdots     & & \vdots\\
    a_{m1}x_1 &+& a_{m2}x_2 &+& \cdots &+& a_{mn}x_n &=& 0
  \end{matrix}
\end{align*}

Todo sistema de ecuaciones lineales homogéneo es consistente, ya que siempre
existe la \textbf{\textit{solución trivial}} (es decir, $x_1=1$, $x_2=1$,
\ldots, $x_n=1=0$).
Debido a que un sistema lineal homogéneo siempre tiene la solución trivial,
entonces para sus soluciones sólo hay dos posibilidades.

\begin{concept}
  \begin{itemize}
    \item El sistema tiene sólo la solución trivial.
    \item El sistema tiene infinidad de soluciones además de la solución
      trivial.
  \end{itemize}
\end{concept}

\begin{theorem}
  Un sistema de ecuaciones lineales homogéneo con más incógnitas que
  ecuaciones tiene infinidad de soluciones
  \label{theo:1}
\end{theorem}

%------------------------------------------
\subsubsection{Matrices y operaciones con matrices}

\begin{concept}[i]
  Una \textbf{matriz} es un arreglo rectangular de números. Los números en el arreglo
  se denominan \textbf{elementos} de la matriz.
\end{concept}

Una matriz general $m\times n$ puede expresarse como:
\begin{align*}
  A = \begin{bmatrix}
    a_{11}& a_{12}& \cdots& a_{1n}\\
    a_{21}& a_{22}& \cdots& a_{2n}\\
    \vdots&\vdots &       &\vdots \\
    a_{m1}& a_{m2}& \cdots& a_{mn}
  \end{bmatrix}
  = \left[ a_{ij} \right]_{m\times n} = \left[ a_{ij} \right]
\end{align*}

A continuación definiremos en que consiste una matriz que se encuentra en su
\bi{forma escalonada reducida}:
\begin{concept}[i]
  \begin{enumerate}
    \item Si una fila no consta completamente de ceros, entonces el primer
      número diferente de cero en la fila es un 1. (Que se denomina
      \textbf{1 principal}).
    \item Si hay filas que constan completamente de ceros, se agrupan en
      la parte inferior de la matriz.
    \item En dos filas consecutivas cualesquiera que no consten completamente
      de ceros, el 1 principal de la fila inferior aparece más a la derecha
      que el 1 principal de la fila superior.
    \item Cada columna que contenga un 1 principal tiene cero en todas las
      demás posiciones.
  \end{enumerate}
\end{concept}

%------------------------------------------
\subsubsection{Operaciones con matrices}

\begin{concept}[i]
  Dos matrices son \textbf{iguales} si tienen el mismo tamaño y sus elementos
  correspondientes son iguales.
\end{concept}

\begin{concept}[i]
  Si $A$ y $B$ son matrices del mismo tamaño, entonces la \textbf{suma}
  $A+B$ es la matriz obtenida al sumar los elementos de $B$ con los elementos
  correspondientes de $A$. No es posible sumar o restar matrices de tamaños
  diferentes.
\end{concept}
En notación matricial:
  \begin{align*}
    (A+B)_{ij} = (A)_{ij} + (B)_{ij} = a_{ij} + b_{ij} \\
    (A-B)_{ij} = (A)_{ij} - (B)_{ij} = a_{ij} - b_{ij}
  \end{align*}

\begin{concept}[i]
  Si $A$ es cualquier matriz y $c$ es cualquier escalar, entonces el
  \textbf{producto} $cA$ es la matriz obtenida al multiplicar cada elemento de
  $A$ por $c$.
\end{concept}
En notación matricial:
\begin{equation*}
  (cA)_{ij} = c(A)_{ij} = ca_{ij}
\end{equation*}

\begin{concept}
  Si $A$ es una matriz $m\times r$ y $B$ es una matriz $r\times n$, entonces
  el \textbf{producto} $AB$ es la matriz $m\times n$ cuyos elementos se
  determinan como sigue. Para encontrar el elemento en la fila $i$ y en la
  columna $j$ de $AB$, considerar sólo la fila $i$ de la Matriz $A$ y la
  columna $j$ de la matriz $B$. Multiplicar entre si los elementos
  correspondientes del renglón y de la columna mencionados y luego sumar los
  productos resultantes.
\end{concept}
En notación matricial:
\begin{align*}
  \left[ (AB)_{ij} \right]_{m\times n} = \sum_{k=1}^r A_{ik}B_{kj}
\end{align*}
La matriz resultante será de $m\times n$.

%------------------------------------------
\subsubsection{Multiplicación de matrices por columnas y por renglones}

\begin{align*}
  j^{th} \textrm{ matriz columna de } AB &= A \left[ j^{th} \textrm{matriz columa de} B
  \right] \\
  i^{th}\textrm{ matriz fila de } AB &= \left[ j^{th} \textrm{ matriz columa de } A
  \right] B
\end{align*}

Si $\textbf{a}_1$, $\textbf{a}_2$, \ldots, $\textbf{a}_m$ denotan las matrices
fila de $A$ y $\textbf{b}_1$, $\textbf{b}_2$, \ldots, $\textbf{b}_n$ denotan
las matrices columna de $B$, entonces por lo establecido recién se concluye
que:

\begin{align*}
  AB =
  A\begin{bmatrix}\textbf{b}_1&\textbf{b}_2&\cdots&\textbf{b}_n \end{bmatrix}
    =
    \begin{bmatrix}A\textbf{b}_1&A\textbf{b}_2&\cdots&A\textbf{b}_n\end{bmatrix}
\end{align*}

\begin{align*}
  AB = \begin{bmatrix}
    \textbf{a}_1 \\
    \textbf{a}_2 \\
    \vdots       \\
    \textbf{a}_m
  \end{bmatrix} B = \begin{bmatrix}
    \textbf{a}_1B \\
    \textbf{a}_2B \\
    \vdots        \\
    \textbf{a}_mB
  \end{bmatrix}
\end{align*}

%------------------------------------------
\subsubsection{Productos de matrices como combinaciones lineales}

Sean:

\begin{align*}
  A = \begin{bmatrix}
    a_{11}& a_{12}& \cdots& a_{1n}\\
    a_{21}& a_{22}& \cdots& a_{2n}\\
    \vdots&\vdots &       &\vdots \\
    a_{m1}& a_{m2}& \cdots& a_{mn}
  \end{bmatrix} &
  & \textbf{x} = \begin{bmatrix}
    x_1    \\
    x_2    \\
    \vdots \\
    x_m
  \end{bmatrix} &
  & \textbf{b} = \begin{bmatrix}
    b_1    \\
    b_2    \\
    \vdots \\
    b_m
  \end{bmatrix} &
\end{align*}

Mediante esta elección es posible expresar al sistema de ecuaciones:
\begin{align*}
  \begin{matrix}
    a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n &=& b_1     \\
    a_{21}x_1 &+& a_{22}x_2 &+& \cdots &+& a_{2n}x_n &=& b_2     \\
    \vdots    & &\vdots     & &        & &\vdots     & & \vdots\\
    a_{m1}x_1 &+& a_{m2}x_2 &+& \cdots &+& a_{mn}x_n &=& b_m
  \end{matrix}
\end{align*}
como:
\begin{align*}
  A\textbf{x}=\textbf{b}
\end{align*}
La matriz $A$ se denomina \textbf{\emph{matriz de coeficientes}} del sistema.


%------------------------------------------
\subsubsection{Transpuesta de una matriz}

\begin{concept}[i]
  Si $A$ es cualquier matriz $m\times n$, entonces la \textbf{transpuesta de
    $A$}, denotada por $A^T$, se define como la matriz $n\times m$ que se
    obtiene al intercambiar las filas y las columnas de $A$, es decir, la
    primera columna de $A^T$ es la primer fila de $A$, la segunda columna de
    $A^T$ es la segunda fila de $A$, y así sucesivamente.
\end{concept}
En notación matricial:
\begin{align*}
  \left( A^T \right)_{ij} = (A)_{ji}
\end{align*}

%------------------------------------------
\subsubsection{Traza de una matriz}

\begin{concept}[i]
  Si $A$ es una matriz cuadrada, entonces la \textbf{\emph{traza de A}},
  denotada por $\tr (A)$, se define como la suma de la diagonal principal de
  $A$. La traza de $A$ no está definida si $A$ no es una matriz cuadrada.
\end{concept}
En notación matricial:
\begin{align*}
  \tr (A)_{n\times n}=\sum_{i=1}^n a_{ii}
\end{align*}

%------------------------------------------
%------------------------------------------
\subsection{Reglas de la aritmética de matrices}

%------------------------------------------
\subsubsection{Propiedades de las operaciones con matrices}

Muchas de las reglas básicas de la aritmética de los números reales también se
cumplen para matrices, aunque unas cuantas no. Por ejemplo, para números
reales $a$ y $b$ siempre se cumple que $ab=ba$ (\emph{ley conmutativa de la
multiplicación}). Para matrices, sin embargo, $AB$ y $BA$ no necesariamente
son iguales.
\begin{theorem}
  Suponiendo que los tamaños de las matrices son tales que las operaciones
  indicadas se pueden efectuar, entonces son válidas las siguientes reglas de
  aritmética matricial.

  \begin{enumerate}[(a)]
    \item $A+B=B+A$ \hfill \bi{Ley conmutativa de la adición}
    \item $A+(B+C)=(A+B)+C$ \hfill \bi{Ley asociativa de la adición}
    \item $A(BC)=(AB)C$ \hfill \bi{Ley asociativa de la multiplicación}
    \item $A(B+C)=AB+AC$ \hfill \bi{Ley distributiva por la izquierda}
    \item $(B+C)A=BA+CA$ \hfill \bi{Ley distributiva por la derecha}
    \item $A(B-C)=AB-AC$
    \item $(B-C)A=BA-CA$
    \item $a(B+C)=aB+aC$
    \item $a(B-C)=aB-aC$
    \item $(a+b)C=aC+bC$
    \item $(a-b)C=aC-bC$
    \item $a(bC)=(ab)C$
    \item $a(BC)=(aB)C=B(aC)$
  \end{enumerate}
  \label{theo:aritmat}
\end{theorem}

\demo Probaremos el inciso \emph{(d)}. Para ello es necesario probar que
$A(B+C)$ y $AB+AC$ son del mismo tamaño y que los elementos correspondientes
son iguales. Para formar $A(B+C)$, $B$ y $C$ deben ser del mismo tamaño, por
ejemplo, $n\times m$. Entonces $A$ debe tener el mismo número de columnas para
que la multiplicación pueda llevarse a cabo, digamos por ejemplo, $r\times n$
de modo que $A(B+C)$ tendrá dimensiones $r\times m$. Veamos ahora la otra
igualdad. Con estas definiciones para $A$, $B$ y $C$ se cumple que tanto
$AB$ como $AC$ tienen dimensiones de $r\times n$. Por lo tanto $A(B+C)$ y
$AB+AC$ son del mismo tamaño.
Queda entonces probar que los elementos correspondientes de $A(B+C)$ y $AB+AC$
son iguales, es decir que:
\begin{align*}
  [A(B+C)]_{ij}=[AB+AC]_{ij}
\end{align*}
para todos los valores de $i$ y $j$. Por las definiciones de adición y de
multiplicación de matrices se tiene:
\begin{align*}
  \left[ A(B+C)_{ij}
  \right]=&a_{i1}(b_{1j}+c_{1j})+a_{i2}(b_{2j}+c_{2j})+\cdots+a_{im}(b_{mj}+c_{mj}) \\
         =&a_{i1}b_{1j}+a_{i1}c_{1j}+a_{i2}b_{2j}+a_{i2}c_{2j}+\cdots+a_{im}b_{mj}+a_{im}c_{mj}
\end{align*}
Pero:
\begin{align*}
  a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{im}b_{mj}=[AB]_{ij} \\
  a_{i1}c_{1j}+a_{i2}c_{2j}+\cdots+a_{im}c_{mj}=[AC]_{ij}
\end{align*}
Por lo tanto:
\begin{align*}
  [A(B+C)]_{ij}=&[AB]_{ij}+[AC]_{ij} \\
               =&[AB+AC]_{ij}
\end{align*}
Con esto queda demostrado que los elementos correspondientes de $A(B+C)$ y
$AB+AC$ son iguales.

La demostración del inciso \emph{c} es más complicada.

%------------------------------------------
\subsubsection{Matrices cero}

\begin{concept}
  Una matriz que tiene sus elementos iguales a cero se denomina \bi{matriz
  cero}.
\end{concept}

Como ya se sabe que algunas de las reglas de la aritmética para los números reales
no se cumplen en la aritmética matricial, es temerario asumir que todas las
propiedades del número real cero se cumplen para las matrices cero. En la
aritmética de números reales se cumple que:
\begin{itemize}
  \item Si $ab=ac$ y $a\ne 0$, entonces $b=c$ (\emph{ley de cancelación}).
  \item Si $ad=0$ entonces por lo menos uno de los factores del miembro
    izquierdo es cero.
\end{itemize}
En general los resultados correspondientes \textbf{no} son ciertos en aritmética
matricial.

A pesar de lo dicho anteriormente, existen varias propiedades conocidas de
número real cero que \emph{se cumplen} en las matrices cero:
\begin{theorem}
  Suponiendo que los tamaños de las matrices son tales que las operaciones
  indicadas se pueden efectuar, entonces son válidas las siguientes reglas de
  aritmética matricial.
  \begin{enumerate}[(a)]
    \item $A+0=0+A=A$
    \item $A-A=0$
    \item $0-A=-A$
    \item $A0=0$; $0A=0$
  \end{enumerate}
  \label{theo:cero}
\end{theorem}

%------------------------------------------
\subsubsection{Matrices identidad}

\begin{concept}
  Una matriz \textit{cuadrada} que tiene unos en la diagonal principal y ceros
  fuera de ésta se denomina \bi{matriz identidad}.
\end{concept}

En aritmética matricial la matriz identidad juega un papel bastante semejante
al que desempeña el número 1 en las relaciones numéricas $a\cdot1=1\cdot a=a$:
\begin{align*}
  AI=A \qquad \mbox{y} \qquad IA=A
\end{align*}

Como se muestra en el teorema siguiente, las matrices identidad surgen
naturalmente en el estudio de formas escalonadas reducidas de matrices
\emph{cuadradas}.

\begin{theorem}
  Si $R$ es la forma escalonada reducida de una matriz $A$ de $n\times n$,
  entonces $R$ tiene un renglón de ceros, o bien, $R$ es la matriz identidad
  $I_n$.
  \label{theo:matriz_identidad}
\end{theorem}

%------------------------------------------
\subsubsection{Inversa de una matriz}

\begin{concept}
  Si $A$ es una matriz cuadrada y si se puede encontrar una matriz $B$ del
  mismo tamaño tal que $AB=BA=I$, entonces se dice que $A$ es
  \bi{invertible} y $B$ se denomina \bi{inversa} de $A$
\end{concept}

%------------------------------------------
\subsubsection{Propiedades de las inversas}

\begin{theorem}
  Si $B$ y $C$ son, ambas, inversas de la matriz $A$, entonces $B=C$
  \label{theo:propinv}
\end{theorem}

\demo Ya que $B$ es inversa de $A$ se tiene que:
\begin{align*}
     BA=&I && \mbox{(\emph{multiplicando por derecha} por $C$ ambos miembros)}\\
  (BA)C=&IC&& \mbox{(ley asociativa de la multiplicación)}\\
  B(CA)=&C && \\
     BI=&C && \mbox{(suposición inicial)} \\
      B=&C  
\end{align*}

Otra propiedad de las inversas:
\begin{align*}
  AA^{-1}=I \qquad \mbox{y} \qquad A^{-1}A=I
\end{align*}

\begin{theorem}
  Si $A$ y $B$ son matrices invertibles del mismo tamaño, entonces:
  \begin{enumerate}[(a)]
    \item $AB$ es invertible
    \item $(AB)^{-1}=B^{-1}A^{-1}$
  \end{enumerate}
  \label{theo:compinv}
\end{theorem}

\demo Si se puede demostrar que:
\begin{align*}
  (AB)(B^{-1}A^{-1})=(B^{-1}A^{-1})(AB)=I
\end{align*}
entonces se habrá demostrado simultáneamente que la matriz $AB$ es invertible
y que $(AB)^{-1}=B^{-1}A^{-1}$.
Siguiendo el siguiente razonamiento:
\begin{align*}
  (AB)(B^{-1}A^{-1})=&(B^{-1}A^{-1})(AB) \\
  A(BB^{-1})A^{-1}=&B^{-1}(A^{-1}A)B \\
  AIA^{-1}=&B^{-1}IB \\
  AA^{-1}=&B^{-1}B \\
  I=&I
\end{align*}

Aunque la siguiente declaración no se demostrará, este último concepto se
puede extender para incluir tres o más factores.
\begin{concept}[i]
  Un producto de cualquier número de matrices invertibles es invertible, y la
  inversa del producto es el producto de las inversas en orden invertido.
\end{concept}

%------------------------------------------
\subsubsection{Potencias de una matriz}
\begin{concept}
  Si $A$ es una matriz cuadrada, entonces las potencias enteras no negativas
  se definen como:
  \begin{align*}
    A^0=I \qquad \mbox{y} \qquad A^n=\prod_i^nA \quad\mbox{con $n>0$}
  \end{align*}
  A su vez, si $A$ es invertible, entonces las potencias negativas de
  $A$ se definen como:
  \begin{align*}
    A^{-n} = \left( A^{-1} \right)^n = \prod_i^nA^{-1} \quad\mbox{con $n>0$}
  \end{align*}
\end{concept}

\begin{theorem}
  Si $A$ es una matriz cuadrada y $r$ y $s$ son enteros, entonces:
  \begin{align*}
    A^rA^s=A^{r+s} \qquad \mbox{y} \qquad (A^r)^s=A^{rs}
  \end{align*}
  \label{theo:expmatr}
\end{theorem}

\begin{theorem}
  Si $A$ es una matriz invertible, entonces:
  \begin{enumerate}[(a)]
    \item $A^{-1}$ es invertible y $(A^{-1})^{-1}=A$.
    \item $A^n$ es invertible y $(A^n)^{-1}=(A^{-1})^n$ para $n=0,1,2,$\ldots
    \item Para cualquier escalar $k$ diferente de cero, la matriz $kA$ es
      invertible y $(kA)^{-1}=\tfrac{1}{k}A^{-1}$.
  \end{enumerate}
  \label{theo:expmatr2}
\end{theorem}

\demo
\begin{enumerate}[(a)]
  \item Como $AA^{-1}=A^{-1}A=I$, la matriz $A^{-1}$ es invertible y
    $(A^{-1})^{-1}=A$.
  \item De la extensión del teorema \ref{theo:compinv} (no demostrado aquí)
    podemos decir que debido a que $A$ es invertible entonces $A^n$ es
    invertible y su inversa es el producto de las inversas en orden invertido.
    Esto es:
    \begin{align*}
      \left( A^n \right)^{-1} &= \left( A^{-1} \right)^n \\
      &= A^{-n}
    \end{align*}
  \item Si $k$ es cualquier escalar diferente de cero, entonces por los
    resultados \emph{(l)} y \emph{(m)} del teorema \ref{theo:aritmat} es
    posible escribir:
    \begin{align*}
      (kA)\left( \frac{1}{k}A^{-1} \right) &= \frac{1}{k}(kA)A^{-1} \\
                                            &=\left( \frac{1}{k}k \right)AA^{-1} \\
                                            &= 1I \\
                                            &= I
    \end{align*}
\end{enumerate}


%------------------------------------------
\subsubsection{Propiedades de la transpuesta}

\begin{theorem}
  Si los tamaños de las matrices son tales que se pueden efectuar las
  operaciones planteadas, entonces:
  \begin{enumerate}[(a)]
    \item $\left( (A)^T \right) ^T=A$
    \item $(A+B)^T=A^T+B^T$ y $(A-B)^T=A^T-B^T$
    \item $(kA)^T=kA^T$, donde $k$ es cualquier escalar
    \item $(AB)^T=B^TA^T$
  \end{enumerate}
  \label{theo:operactrans}
\end{theorem}

\demo Al considerar que al transponer una matriz se intercambian sus filas por
sus columnas, los incisos \emph{(a)}, \emph{(b)} y \emph{(c)} deben ser
evidentes. Para demostrar el inciso \emph{(d)} supongamos:
\begin{align*}
  A=\left[ a_{ij} \right]_{m\times r} \qquad \mbox{y} \qquad B=\left[ b_{ij} \right]_{r\times n}
\end{align*}
A continuación demostraremos que $(AB)^T$ y $B^TA^T$ son del mismo tamaño. El
producto $AB$ tendrá un tamaño de $m\times n$. Ahora bien, la transpuesta del
producto $(AB)^T$ tendrá dimensiones $n\times m$. Por otro lado:
\begin{align*}
  A^T=\left[ a'_{ij} \right]_{r\times m} \qquad \mbox{y} \qquad B^T=\left[ b'_{ij} \right]_{n\times r}
\end{align*}
De modo que el producto $B^TA^T$ tendrá un tamaño $n\times m$. Por lo que
$(AB)^T$ y $B^TA^T$ tienen ambos el mismo tamaño.
Nos queda demostrar que los elementos correspondientes de $(AB)^T$ y $B^TA^T$
son los mismos, es decir:
\begin{align*}
  \left( (AB)^T \right)_{ij} = \left( B^TA^T \right)_{ij}
\end{align*}
Para encontrar $\left( (AB)^T \right)_{ij}$ primero encontramos el producto
$AB$:
\begin{align*}
  (AB)_{ij}&= a_{i1}b_{1j}+a_{i2}b_{2j}+\ldots+a_{ir}b_{rj} \\
  &= \sum_{k=1}^r a_{ik}b_{kj}
\end{align*}
La transpuesta de este último resultado es:
\begin{align*}
  \left( (AB)^T \right)_{ij} &= (AB)_{ji} \\
  &= \sum_{k=1}^r a_{jk}b_{ki}
\end{align*}
Notar el intercambio de subíndices con respecto al último resultado.
Analicemos ahora el otro miembro de la igualdad $\left( B^TA^T \right)_{ij}$.
\begin{align*}
  \left( B^TA^T \right)_{ij} &= \sum_{k=1}^r b'_{ik}a'_{kj} \\
  &= \sum_{k=1}^r b_{ki}a_{jk} \\
  &= \sum_{k=1}^r a_{jk}b_{ki}
\end{align*}
Con esto queda demostrado que $(AB)^T$ y $B^TA^T$ son iguales.

Aunque no se demostrará el siguiente hecho, el inciso \emph{(d)} del último
teorema se puede extender para incluir tres o más factores:
\begin{concept}[i]
  La transpuesta de un producto de cualquier número de matrices es igual al
  producto de sus transpuestas en orden invertido.
\end{concept}

%------------------------------------------
\subsubsection{Invertibilidad de una transpuesta}

\begin{theorem}
  Si $A$ es una matriz invertible, entonces $A^T$ también es invertible y:
  \begin{align}
    \left( A^T \right)^{-1} = \left( A^{-1} \right)^T
    \label{eqn:invtrans}
  \end{align}
  \label{theo:invtrans}
\end{theorem}

\demo Se puede probar la invertibilidad de $A^T$ y obtener \eqref{eqn:invtrans}
al demostrar que:
\begin{align*}
  A^T\left( A^{-1} \right)^T = \left( A^{-1} \right)^T A^T = I
\end{align*}
Por el inciso \emph{(d)} del teorema \ref{theo:operactrans}:
\begin{align*}
  \left( A^T\left( A^{-1} \right)^T \right)^T &= \left( \left( A^{-1} \right)^T
  A^T \right)^T \\
  \left( \left( A^{-1} \right)^T \right)^T \left( A^T \right)^T &= \left( A^T
  \right)^T \left( \left( A^{-1} \right)^T \right)^T \\
  A^{-1}A &= AA^{-1} \\
  I &= I
\end{align*}


%------------------------------------------
\subsection{Matrices elementales y un método para determinar $A^{-1}$}

%------------------------------------------
\subsubsection{Matrices elementales}

\begin{concept}
  Una matriz $n\times n$ se denomina \bi{matriz elemental} si se puede obtener
  a partir de la matriz identidad $I_n$ al efectuar una sola operación
  elemental en las filas.
\end{concept}

\begin{theorem}
  Si la matriz elemental $E$ resulta de la ejecución de ciertas operaciones en
  los renglones de $I_m$ y si $A$ es una matriz $m\times n$, entonces el
  producto $EA$ es la matriz que se obtiene cuando la misma operación en las
  filas se efectúa en $A$.
  \label{theo:matr_elem}
\end{theorem}

\begin{theorem}
  Toda matriz elemental es invertible, y la inversa es también una matriz
  elemental
  \label{theo:elematr}
\end{theorem}

\demo Si $E$ es una matriz elemental, entonces $E$ se obtiene al efectuar
algunas operaciones en las filas de $I$. Sea $E_0$ la matriz que se obtiene
cuando la inversa de esta operación se efectúa en I. Entonces usando el hecho
de que las operaciones inversas en las filas cancelan mutuamente su efecto, se
concluye que:
\begin{align*}
  E_0E=I \qquad \mbox{y} \qquad EE_0=I
\end{align*}

El siguiente teorema establece algunas relaciones fundamentales entre
invertibilidad, sistemas lineales homogéneos, formas escalonadas reducidas y
matrices elementales.

\begin{theorem}
  Si $A$ es una matriz $n\times n$, entonces las siguientes proposiciones son
  equivalentes, es decir, todas son verdaderas o todas son falsas.
  \begin{enumerate}[(a)]
    \item $A$ es invertible.
    \item $A\mathbf{x}=\textbf{0}$ sólo tiene la solución trivial.
    \item La forma escalonada reducida de $A$ es $I_n$.
    \item A se puede expresar como un producto de matrices elementales.
  \end{enumerate}
  \label{theo:relacfund}
\end{theorem}

\demo Se demostrará la equivalencia estableciendo la cadena de implicaciones
$a\Rightarrow b\Rightarrow c\Rightarrow d\Rightarrow a$.

$a\Rightarrow b$: Si $A$ es invertible y sea  $\mathbf{x}_0$ cualquier solución de
$A\mathbf{x}=\textbf{0}$; así, $A\mathbf{x}_0=0$. Al multiplicar ambos miembros de la
ecuación por la matriz $A^{-1}$ se obtiene:
\begin{align*}
  A^{-1}(A\mathbf{x}_0) &= A^{-1}\textbf{0} \\
  (A^{-1}A)\mathbf{x}_0 &= \textbf{0} \\
  I\mathbf{x}_0 &= 0 \\
  \mathbf{x}_0 &= 0
\end{align*}
Por lo tanto $A\mathbf{x}_0=\textbf{0}$ sólo tiene la solución trivial.

$b\Rightarrow c$: Sea $A\mathbf{x}=\textbf{0}$. La matriz:
\begin{align*}
  \begin{bmatrix}
    a_{11}& a_{12}& \cdots& a_{1n} \\
    a_{21}& a_{22}& \cdots& a_{2n} \\
    \vdots&\vdots &       &\vdots  \\
    a_{m1}& a_{m2}& \cdots& a_{mn} 
  \end{bmatrix}
\end{align*}
puede llevarse por medio de operaciones elementales en las filas a:
\begin{align*}
  \begin{bmatrix}
    1&   0& \cdots& 0 \\
    0&   1& \cdots& 0 \\
    \vdots&\vdots & \ddots  &\vdots  \\
    0&   0& \cdots& 1 
  \end{bmatrix}
\end{align*}
Por lo tanto la forma escalonada reducida de $A$ es la matriz $I_n$.

$c\Rightarrow d$: Suponer que la forma escalonada reducida de $A$ es
$I_n$ implica que $A$ se puede reducir a $I_n$ mediante una sucesión finita de
operaciones elementales en las filas. Esto es lo mismo que escribir:
\begin{align}
  E_k\cdots E_2E_1A=I_n
  \label{eqn:oem}
\end{align}
Por el teorema \ref{theo:elematr}, las matrices $E_k\cdots E_2E_1A=I_n$ son
invertibles. Al multiplicar por izquierda a ambos miembros de la ecuación
\eqref{eqn:oem} se obtiene:
\begin{align*}
  A=E_1^{-1}E_2^{-1}\cdots E_k^{-1}I_n=E_1^{-1}E_2^{-1}\cdots E_k^{-1}
\end{align*}
Por lo que queda demostrado que $A$ puede expresarse como un producto de
matrices elementales.

$d\Rightarrow a$: Si $A$ es un producto de matrices elementales entonces por
los teoremas \ref{theo:compinv} y \ref{theo:elematr} la matriz es un producto
de matrices invertibles, y por lo tanto es invertible.

%------------------------------------------
\subsubsection{Equivalencia por renglones}
Las matrices que se pueden obtener a partir de otra matriz mediante la
ejecución de una sucesión finita de operaciones elementales en las filas se
denominan \bi{equivalentes por renglones}. Con esta terminología por los
incisos \emph{(a)} y \emph{(c)} del teorema \ref{theo:relacfund} se concluye
que una matriz $A_{n\times n}$ es invertible si y sólo si es equivalentes por
renglones a la matriz identidad $I_{n}$.

%------------------------------------------
\subsubsection{Un método para invertir matrices}

\begin{concept}[i]
  Para determinar la inversa de una matriz invertible $A$, es necesario
  encontrar una sucesión de operaciones elementales en las filas que reduzca a
  $A$ a la matriz identidad y luego efectuar esta misma sucesión de
  operaciones en $I_n$ para obtener $A_{-{1}}$.
\end{concept}

%------------------------------------------
%------------------------------------------
\subsection{Otros resultados sobre sistemas de ecuaciones e invertibilidad}

\begin{theorem}
  Si $A$ es una matriz invertible $n\times n$, entonces para toda matriz
  $\mathbf{b}$ $n\times 1$, el sistema de ecuaciones $A\mathbf{x}=\mathbf{b}$
  tiene exactamente una solución; a saber, $\mathbf{x}=A^{-1}\mathbf{b}$.
  \label{theo:resol_sist_lineales}
\end{theorem}

\demo
\begin{align*}
  A\mathbf{x}&=\mathbf{b} \\
  A^{-1}(A\mathbf{x})&=A^{-1}\mathbf{b} \\
  \left( A^{-1}A \right)\mathbf{x}&=A^{-1}\mathbf{b} \\
  \mathbf{x}&=A^{-1}\mathbf{b}
\end{align*}
Para demostrar que esta es la única solución se supondrá que $\mathbf{x}_0$ es
una solución arbitraria y luego se demostrará que $\mathbf{x}_0$ debe ser
$A^{-1}\mathbf{b}$.

Si $\mathbf{x}_0$ es cualquier solución, entonces $A\mathbf{x}_0=\mathbf{b}$.
Al multiplicar ambos miembros por $A^{-1}$ se obtiene
$\mathbf{x}_0=A^{-1}\mathbf{b}$.

%------------------------------------------
\subsection{Matrices diagonales, triangulares y simétricas}

%------------------------------------------
\subsubsection{Matrices diagonales}

\begin{concept}
  Una matriz cuadrada en la que todos los elementos fuera de la diagonal son
  cero se denomina \bi{matriz diagonal} y puede representarse como:
  \begin{align*}
    D=
  \begin{bmatrix}
    d_1&   0& \cdots& 0 \\
    0&   d_2& \cdots& 0 \\
    \vdots&\vdots & \ddots  &\vdots  \\
    0&   0& \cdots& d_n 
  \end{bmatrix}
  \end{align*}
\end{concept}

Una matriz diagonal es invertible sólo si todos los elementos de su diagonal
son distintos de cero, en este caso la inversa es:
\begin{align*}
  D^{-1}=
\begin{bmatrix}
  1/d_1 &   0   & \cdots  & 0 \\
  0     &  1/d_2& \cdots  & 0 \\
  \vdots&\vdots & \ddots  &\vdots  \\
  0     &   0   & \cdots  & 1/d_n 
\end{bmatrix}
\end{align*}

Por otro lado, las potencias de las matrices diagonales son fáciles de
calcular. Sea $k$ un número entero positivo, entonces:
\begin{align*}
  D^k=
\begin{bmatrix}
  d_1^k &   0   & \cdots  & 0 \\
  0     &  d_2^k& \cdots  & 0 \\
  \vdots&\vdots & \ddots  &\vdots  \\
  0     &   0   & \cdots  & d_n^k 
\end{bmatrix}
\end{align*}

Finalmente para multiplicar una matriz $A$ por izquierda por una matriz
diagonal $D$, es posible multiplicar filas sucesivas de $A$ por los elementos
diagonales sucesivos de $D$; y para multiplicar $A$ por la derecha por
$D$ es posible multiplicar columnas sucesivas de $A$ por los elementos
diagonales sucesivos de $D$.

%------------------------------------------
\subsubsection{Matrices triangulares}

\begin{concept}
  Una matriz \emph{cuadrada} en la que todos los elementos arriba de la
  diagonal principal son cero se denomina \bi{triangular inferior}, y una
  matriz \emph{cuadrada} en la que todos los elementos debajo de la
  diagonal principal son cero se denomina \bi{triangular superior}.
\end{concept}

%------------------------------------------
\subsubsection{Matrices simétricas}

\begin{concept}[i]
  Una matriz cuadrada es \bi{simétrica} si:
  \begin{align*}
    A=A^T
  \end{align*}
\end{concept}

\begin{theorem}
  Si $A$ y $B$ son matrices simétricas del mismo tamaño y si $k$ es cualquier
  escalar, entonces:
  \begin{enumerate}[(a)]
    \item $A^T$ es simétrica.
    \item $A+B$ y $A-B$ son simétricas
    \item $kA$ es simétrica.
  \end{enumerate}
  \label{theo:simetmat}
\end{theorem}

\obse En general no es cierto que el producto de matrices simétricas es
simétrico. Para ver esto, por el inciso \emph{(d)} del teorema
\ref{theo:operactrans}, se tiene:
\begin{align*}
  (AB)^T=B^TA^T=BA
\end{align*}
Como $AB$ y $BA$ suelen ser diferentes, se concluye que en términos generales
$AB$ no es simétrico. Sin embargo, en el caso que $AB=BA$, entonces se dice
que $A$ y $B$ \bi{conmutan}. En resumen:
\begin{concept}[i]
  El producto de dos matrices simétricas es simétrico si y sólo si las
  matrices conmutan.
\end{concept}

\begin{theorem}
  Si $A$ es una matriz simétrica invertible, entonces $A^{-1}$ es simétrica.
  \label{theo:matrsiminv}
\end{theorem}

\demo Supongamos que $A$ es simétrica e invertible. Por el teorema
\ref{theo:invtrans} y el hecho de que $A=A^T$, se tiene:
\begin{align*}
  \left( A^{-1} \right)^T=\left( A^T \right)^{-1}=A^{-1}
\end{align*}
lo que demuestra que $A^{-1}$ es simétrica.

%------------------------------------------
\subsubsection{Matrices de la forma $AA^T$ y $A^TA$}

Los productos de matrices de la forma $AA^T$ y $A^TA$ son siempre simétricos
porque:
\begin{align*}
  \left( AA^T \right)^T = \left( A^T \right)^TA^T = AA^T \quad \mbox{y} \quad
  \left( A^TA \right)^T=A^T\left( A^T \right)^T=A^TA
\end{align*}

\begin{theorem}
  Si $A$ es una matriz invertible, entonces $AA^T$ y $A^TA$ también son
  invertibles.
  \label{theo:invATA}
\end{theorem}

\demo Como $A$ es invertible, entonces por el teorema \ref{theo:invtrans}
también lo es $A^T$. Así, $AA^T$ y $A^TA$ son invertibles, ya que son el
producto de matrices invertibles.


\newpage
%------------------------------------------
%------------------------------------------
%------------------------------------------
\section{Determinantes}

%------------------------------------------
%------------------------------------------
\subsection{La función determinante}

%------------------------------------------
\subsubsection{Permutaciones}

\begin{concept}
  Una \bi{permutación} del conjunto de enteros $\left\{ 1,2,\ldots,n \right\}$
  es un arreglo de éstos en algún orden sin omisiones ni repeticiones.
\end{concept}

Una inversión consiste en un cambio en el orden entre dos elementos siempre y
cuando un entero mayor precede a uno menor.

Para calcular el número de inversiones en una permutación
$(j_1,j_2,\ldots,j_n)$ se prosigue de la siguiente forma:
\begin{enumerate}
  \item Encontrar el número de enteros que son menores que $j_1$ y que están
    después de $j_1$ en la permutación.
  \item Encontrar el número de enteros que son menores que $j_2$ y que están
    después de $j_2$ en la permutación.
  \item Continuar este proceso desde $j_3$ hasta $j_{n-1}$.
  \item La suma de estos números es el número total de inversiones que hay en
    la permutación.
\end{enumerate}
Por ejemplo, en la permutación $(6,1,3,4,5,2)$ el número de inversiones es
$5+0+1+1+1=8$. Este número indica que se deben haber realizado 8 inversiones a
partir del conjunto de enteros $(1,2,3,4,5,6)$ para llegar a la permutación
$(6,1,3,4,5,2)$.

Se dice que una permutación es \bi{par} si el número total de inversiones es
un entero par, y es \bi{impar} si el número total de inversiones es un número
entero impar.

Por \bi{producto elemental} de una matriz $A_{n\times n}$ se entiende cualquier producto
de $n$ elementos de $A$, de los cuales ningún par de elementos proviene de la
misma fila o de la misma columna. De esta manera una matriz $A_{n\times n}$
tiene $n!$ productos elementales. Estos productos son de la forma
$a_{1j_1}a_{2j_2}\ldots a_{nj_n}$, donde $(j_1,j_2,\ldots,j_n)$ es una
permutación de conjunto $\left\{ 1,2,3,\ldots,n \right\}$. El signo de cada producto
elemental será positivo si la permutación $(j_1,j_2,\ldots,j_n)$ es par y
negativo si la permutación $(j_1,j_2,\ldots,j_n)$ es impar.

\bigskip

\begin{concept}
  Sea $A$ una matriz cuadrada. La \bi{función determinante} se denota por
  \bi{det}, y $\det(A)$ se define como la suma de los productos elementales
  con signo de $A$. El número $\det(A)$ se denomina \bi{determinante de A}.
\end{concept}

%------------------------------------------
%------------------------------------------
\subsection{Evaluación de determinantes por reducción de filas}

%------------------------------------------
\subsubsection{Un teorema básico}

\begin{theorem}
  Sea $A$ una matriz cuadrada.
  \begin{enumerate}[(a)]
    \item Si $A$ tiene una fila de ceros o una columna de ceros, entonces
      $\det(A)=0$.
    \item $\det(A)=\det(A^T)$.
  \end{enumerate}
  \label{theo:detbasic}
\end{theorem}

\demo \emph{(a)}.Como todo producto elemental con signo de $A$ tiene un factor de cada
fila y un factor de cada columna, entonces todo producto elemental con signo
tiene necesariamente un factor de una fila cero o de una columna cero.
\emph{(b)}. Se omite la demostración de este inciso, pero se recuerda que un
producto elemental tiene un factor de cada fila y un factor de cada columna,
de modo que es evidente que $A$ y $A^T$ tienen exactamente el mismo conjunto
de productos elementales.

%------------------------------------------
\subsubsection{Determinantes de matrices triangulares}

\begin{theorem}
  Si $A$ es una matriz triangular $n\times n$ (triangular superior, inferior o
  diagonal), entonces $\det(A)$ es el producto de los elementos de la diagonal
  principal; es decir:
  \begin{align*}
    \det(A)=\prod_i^n a_{ii}
  \end{align*}
  \label{theo:triangdet}
\end{theorem}

%------------------------------------------
\subsubsection{Elementales en las filas sobre un determinante}

\begin{theorem}
  Sea $A$ una matriz $n\times n$
  \begin{enumerate}[(a)]
    \item Si $B$ es la matriz que se obtiene cuando una sola fila o una sola
      columna de $A$ se multiplica por un escalar $k$, entonces:
      \begin{align*}
        \det(B)=k\det(A)
      \end{align*}
    \item Si $B$ es la matriz que se obtiene cuando se intercambian dos filas
      o dos columnas, entonces:
      \begin{align*}
        \det(B)=-\det(A)
      \end{align*}
    \item Si $B$ es la matriz que se obtiene cuando un múltiplo de una fila de
      $A$ se suma a otra fila o cuando un múltiplo de una columna se suma a
      otra columna, entonces:
      \begin{align*}
        \det(B)=\det(A)
      \end{align*}
  \end{enumerate}
  \label{theo:det_op_elem}
\end{theorem}

%------------------------------------------
\subsubsection{Determinantes de matrices elementales}
\begin{theorem}
  Sea $E$ una matriz elemental $n\times n$.
  \begin{enumerate}[(a)]
    \item Si $E$ se obtiene al multiplicar una fila por $k$ un renglón de
      $I_n$, entonces $\det(E)=k$.
    \item Si $E$ se obtiene al intercambiar dos filas de $I_n$, entonces
      $\det(E)=-1$.
    \item Si $E$ se obtiene al sumar un múltiplo de una fila de $I_n$ a otra
      fila, entonces $\det(E)=1$.
  \end{enumerate}
  \label{theo:det_matr_elem}
\end{theorem}

%------------------------------------------
\subsubsection{Determinantes con filas o columnas proporcionales}

\begin{theorem}
  Si $A$ es una matriz cuadrada con dos filas o dos columnas proporcionales,
  entonces $\det(A)=0$.
  \label{theo:deter_matr_propor}
\end{theorem}

%------------------------------------------
\subsubsection{Propiedades básicas de los determinantes}

\begin{concept}
  \begin{align*}
    \det(kA)=k^n\det(A)
  \end{align*}
\end{concept}

Desafortunadamente, en general no existe una relación simple entre
$\det(A)$, $\det(B)$ y $\det(A+B)$ y:
\begin{align*}
  \det(A) + \det(B) \ne \det(A+B)
\end{align*}

\begin{theorem}
  Sean $A$, $B$ y $C$ matrices $n\times n$ que sólo difieren en una fila, por
  ejemplo, la r-ésima, y suponer que la r-ésima fila de $C$ se puede obtener
  sumando los elementos correspondientes de las r-esimas filas de $A$ y
  $B$. Entonces:
  \begin{align*}
    \det(C)=\det(A)+\det(B)
  \end{align*}
  El mismo resultado es cierto para columnas.
  \label{theo:det_suma}
\end{theorem}

%------------------------------------------
\subsubsection{Determinante de un producto de matrices}

La siguiente afirmación es cierta:
\begin{align}
  \det(AB)=\det(A)\det(B)
  \label{eqn:prod_det}
\end{align}
Como la demostración de este teorema es bastante minuciosa, primero es
necesario desarrollar algunos resultados preliminares. Se comenzará con el
caso en el que $A$ es una matriz elemental. Debido a que este caso especial
sirve como prueba de un teorema mayor (la demostración de
\eqref{eqn:prod_det}), entonces lo denominamos lema:

\begin{lemma}
  Si $B$ es una matriz $n\times n$ y $E$ es una matriz elemental $n\times n$,
  entonces:
  \begin{align*}
    \det(EB)=\det(E)\det(B)
  \end{align*}
  \label{lem:prod_det_1}
\end{lemma}

\demo Se consideran tres casos, cada uno dependiendo de la operación en el
renglón con que se obtiene $E$.

\case{1} Si $E$ se obtiene al multiplicar por $k$ una fila de $I_n$, entonces,
por el teorema \ref{theo:matr_elem}, $EB$ se obtiene a partir de $B$ al
multiplicar por $k$ una fila; así, por el teorema \ref{theo:det_op_elem}a se
tiene que:
\begin{align*}
  \det(EB)=k\det(B)
\end{align*}
Pero por el teorema \ref{theo:det_matr_elem}a se tiene que $\det(E)=k$, de
modo que:
\begin{align*}
  \det(EB)=\det(E)\det(B)
\end{align*}

\case{2 y 3} Las demostraciones de estos casos siguen el mismo patrón que para
el caso anterior.

\obse Por aplicaciones repetidas del lema \ref{lem:prod_det_1} se concluye que
si $B$ es una matriz $n\times n$ y $E_1$, $E_2$ \ldots $E_r$ son matrices
elementales $n\times n$ entonces:
\begin{align}
  \det(E_1E_2\cdots E_rB)=\det(E_1)\det(E_2)\cdots\det(E_r)\det(B)
  \label{eq:det_prod_matr}
\end{align}

%------------------------------------------
\subsubsection{Prueba de la invertibilidad mediante un determinante}

\begin{theorem}
  Una matriz cuadrada $A$ es invertible si y sólo si $\det(A)\ne0$
  \label{theo:proof_inv_matr}
\end{theorem}

\demo Sea $R$ la forma escalonada reducida de $A$.

Como paso preliminar se demostrará que tanto $\det(A)$ como $\det(R)$ son cero
o diferentes de cero. Sean $E_1$, $E_2$ \ldots $E_r$ las matrices elementales
que corresponden a las operaciones elementales en los renglones con que se
obtiene $R$ a partir de $A$. Así:
\begin{align*}
  R=E_r\cdots E_2E_1A
\end{align*}
y según \eqref{eq:det_prod_matr}:
\begin{align}
  \det(R)=\det(E_r)\cdots\det(E_2)\det(E_1)\det(A)
  \label{eqn:det_matr_redu_elem}
\end{align}
Pero por el teorema \ref{theo:det_matr_elem}, los determinantes de las
matrices elementales son diferentes de cero. (Tomar en cuenta que multiplicar
por cero una fila \emph{no} es una operación elemental en los renglones
permitida.) Así por \eqref{eqn:det_matr_redu_elem} se concluye que
$\det(R)$ y $\det(A)$ son cero o diferentes de cero. Ahora se procederá a la
parte importante de la demostración.

Si $A$ es invertible, entonces por el teorema \ref{theo:relacfund} se tiene
que $R=I$, de modo que $\det(R)=1\ne0$ y, en consecuencia, $\det(A)\ne0$.
Recíprocamente, si $\det(A)\ne0$ entonces $\det(R)\ne0$, de modo que $R$ no
puede contener un renglón de ceros. Por el teorema \ref{theo:matriz_identidad}
se concluye que $R=I$, de modo que por el teorema \ref{theo:relacfund} se
tiene que $A$ es invertible.

Por los teoremas \ref{theo:proof_inv_matr} y \ref{theo:deter_matr_propor} se
concluye que una matriz cuadrada con dos filas o columnas proporcionales no es
invertible.

\begin{theorem}
  Si $A$ y $B$ son matrices cuadradas del mismo tamaño, entonces:
  \begin{align*}
    \det(AB)=\det(A)\det(B)
  \end{align*}
  \label{theo:det_prod_matr}
\end{theorem}

\demo La demostración se dividirá en dos casos que dependen de si $A$ es
invertible o no lo es.

\case{1} Si la matriz $A$ no es invertible, entonces por el teorema
\ref{theo:compinv} tampoco lo es el producto $AB$. Así, por por el teorema
\ref{theo:proof_inv_matr} se tiene que $\det(AB)=0$ y $\det(A)=0$, por tanto,
se concluye que:
\begin{align*}
  \det(AB)=\det(A)\det(B)
\end{align*}

\case{2} Ahora se supone que $A$ es invertible. Por el teorema
\ref{theo:relacfund}, la matriz $A$ se puede expresar como producto de
matrices elementales, por ejemplo:
\begin{align}
  A=E_1E_2\cdots E_r
  \label{eqn:matrix_A_elem}
\end{align}
de modo que:
\begin{align*}
  AB=E_1E_2\cdots E_rB
\end{align*}
Si se aplica \eqref{eq:det_prod_matr} a esta ecuación se obtiene:
\begin{align*}
  \det(AB)&=\det(E_1)\det(E_2)\cdots \det(E_r)\det(B) \\
          &=\det(E_1E_2\cdots E_r)\det(B)
\end{align*}
que según \eqref{eqn:matrix_A_elem}, se puede escribir como:
\begin{align*}
  \det(AB)=\det(A)\det(B)
\end{align*}

El siguiente teorema proporciona una relación útil entre el determinante de
una matriz y el determinante de su inversa.

\begin{theorem}
  Si $A$ es invertible, entonces:
  \begin{align*}
    \det\left( A^{-1} \right)=\frac{1}{\det(A)}
  \end{align*}
  \label{theo:det_inv_matriz}
\end{theorem}

\demo 
\begin{align*}
  A^{-1}A&=I \\
  \det\left( A^{-1}A \right)&=\det(I) \\
  \det\left( A^{-1} \right)\det(A)&=1
\end{align*}
como $det(A)\ne0$, la demostración puede completarse dividiendo ambos miembros
por $\det(A)$.

%------------------------------------------
\subsubsection{Sistemas lineales de la forma
$A\mathbf{b}=\lambda\mathbf{b}$}

Muchas aplicaciones del álgebra lineal están relacionadas con sistemas de
$n$ ecuaciones lineales en $n$ incógnitas que se expresan como:
\begin{align}
  A\mathbf{x}&=\lambda\mathbf{x} \\
  (\lambda I-A)\mathbf{x}&=\textbf{0} \label{eqn:sist_ec_lineales}
\end{align}

El problema de interés esencial en sistemas lineales de la forma
\eqref{eqn:sist_ec_lineales} es determinar los valores de $\lambda$ para los
cuales el sistema tiene una solución no trivial; ese valor de $\lambda$ se
denomina \bi{eingenvalor} o \bi{autovalor} de $A$. Si $\lambda$ es un
autovalor de $A$, entonces las soluciones no triviales de
\eqref{eqn:sist_ec_lineales} se denominan \bi{eingenvectores} o
\bi{autovectores} de $A$ correspondientes a $\lambda$.

De acuerdo con el teorema \ref{theo:proof_inv_matr} se concluye que el sistema
$(\lambda I-A)\mathbf{x}=\mathbf{0}$ tiene una solución no trivial si y sólo si:
\begin{align}
  \det(\lambda I-A)=0
  \label{eqn:ec_caracteristica}
\end{align}
Ésta se denomina \bi{ecuación característica} de $A$; los autovalores de
$A$ se pueden encontrar resolviendo esta ecuación para $\lambda$. Notar que la
matriz $(\lambda I-A)$ no es invertible.

%------------------------------------------
%------------------------------------------
\subsection{Desarrollo por cofactores}

%------------------------------------------
\subsubsection{Menores y cofactores}

\begin{concept}
  Si $A$ es una matriz cuadrada, entonces el \bi{menor del elemento $a_{ij}$}
  se denota por $M_{ij}$ y se define como el determinante de la submatriz que
  queda después de quitar la $i$-ésima fila y la $j$-ésima columna de
  $A$. El número $(-1)^{i+j}M_{ij}$ se denota por $C_{ij}$ y se denomina
  \bi{cofactor del elemento $a_{ij}$}.
\end{concept}

\begin{theorem}
  El determinante de una matriz $A_{n\times n}$ se puede calcular
  multiplicando los elementos de cualquier fila (o de cualquier columna) por
  sus cofactores y sumando los productos resultantes; es decir, para cada
  $1\le i\le n$ y $1 \le j \le n$, se tiene que:
  \begin{align*}
    \det(A)&=\sum_{i=1}^n a_{ij}C_{ij} \quad \mbox{desarrollo por cofactores a
    lo largo de la $j$-ésima columna} \\
    \det(A)&=\sum_{j=1}^n a_{ij}C_{ij} \quad \mbox{desarrollo por cofactores a
    lo largo de la $i$-ésima fila}
  \end{align*}
  \label{theo:det_des_cof}
\end{theorem}

%------------------------------------------
\subsubsection{Adjunta de una matriz}

Aunque no se demuestra la siguiente afirmación es verdadera:
\begin{concept}
  Resulta que si los elementos de cualquier fila se multiplican por cofactores
  correspondientes de una fila \emph{diferente}, la suma de tales productos
  siempre es cero. Este resultado también se aplica para columnas.
\end{concept}

\begin{concept}
  Si $A$ es cualquier matriz $n\times n$ y $C_{ij}$ es el cofactor de
  $a_{ij}$, entonces la matriz:
  \begin{align*}
    \begin{bmatrix}
      C_{11}& C_{12}& \cdots& C_{1n}\\
      C_{21}& C_{22}& \cdots& C_{2n}\\
      \vdots&\vdots &       &\vdots \\
      C_{n1}& C_{n2}& \cdots& C_{nn}
    \end{bmatrix}
  \end{align*}
  se denomina \bi{matriz de cofactores de $A$}. La transpuesta de esta matriz
  se denomina \bi{adjunta de A} y se denota por $\adj(A)$.
\end{concept}

%------------------------------------------
\subsubsection{Fórmula para la inversa de una matriz}

\begin{theorem}
  Si $A$ es una matriz invertible, entonces
  \begin{align*}
    A^{-1}=\frac{1}{\det(A)}\adj(A)
  \end{align*}
  \label{theo:inv_adjunta}
\end{theorem}

\demo Primero se demostrará que:
\begin{align*}
  A\adj(A)=\det(A)I
\end{align*}
El elemento en la $i$-ésima fila y la $j$-ésima columna de $A\adj(A)$ es:
\begin{align}
  \left( A\adj(A) \right)_{ij}=\sum_{k=1}^n a_{ik}C_{jk}
  \label{eqn:AAdj}
\end{align}
Si $i=j$, entonces \eqref{eqn:AAdj} es el desarrollo por cofactores de
$\det(A)$ a lo largo de la $i$-ésima fila de $A$ (teorema
\ref{theo:det_des_cof}), y si $i\ne j$ entonces las letras $a$ y los
cofactores provienen de filas diferentes de $A$, de modo que el valor de
\eqref{eqn:AAdj} es cero. En consecuencia:
\begin{align*}
  A\adj(A)=
    \begin{bmatrix}
      \det(A)& 0& \cdots& 0\\
      0& \det(A)& \cdots& 0\\
      \vdots&\vdots &       &\vdots \\
      0& 0& \cdots& \det(A)
    \end{bmatrix}
    = \det(A)I
\end{align*}
Dado que $A$ es invertible, $\det(A)\ne0$. Por tanto la ecuación
\eqref{eqn:AAdjM} puede volver a escribirse como:
\begin{align*}
  \frac{1}{\det(A)}\left[ A\adj(A) \right]&=I \\
  A\left[ \frac{1}{\det(A)}\adj(A) \right]&=I
\end{align*}
Multiplicando por la izquierda ambos miembros por $A^{-1}$, se obtiene:
\begin{align*}
    A^{-1}=\frac{1}{\det(A)}\adj(A)
\end{align*}

%------------------------------------------
%------------------------------------------
\subsection{Resumen}

En el siguiente teorema se resumen muchos teoremas previos.

\begin{theorem}
  Si $A$ es una matriz $n\times n$, entonces las siguientes proposiciones son
  equivalentes:
  \begin{enumerate}[(a)]
    \item $A$ es invertible.
    \item $A\mathbf{x}=\mathbf{0}$ sólo tiene la solución trivial.
    \item La forma escalonada reducida de $A$ es $I_n$.
    \item $A$ se puede expresar como un producto de matrices elementales.
    \item $A\mathbf{x}=\mathbf{b}$ es consistente para toda matriz
      $\mathbf{b}$ $n\times 1$.
    \item $A\mathbf{x}=\mathbf{b}$ tiene exactamente una solución para toda
      matriz $\mathbf{b}$ $n\times 1$.
    \item $\det(A)=0$.
  \end{enumerate}
  \label{theo:resumen_matriz_det}
\end{theorem}

\newpage

%------------------------------------------
%------------------------------------------
%------------------------------------------
\section{Espacios vectoriales euclidianos}

\subsection{Espacio euclidiano $n$ dimensional}

\subsubsection{vectores en el espacio $n$ dimensional}

\begin{concept}
  Si $n$ es un entero positivo, entonces una \bi{$n$-ada ordenada} es una
  sucesión de $n$ números reales $(a_1,a_2,\ldots ,a_n)$. El conjunto de todas
  las $n$-adas ordenadas se denomina espacio $n$ dimensional y se denota por
  $R^n$
\end{concept}

\begin{concept}
  Dos vectores $\mathbf{u}=(u_1,u_2,\ldots,u_n)$ y
  $\mathbf{v}=(v_1,v_2,\ldots,v_n)$ en $R^n$ se denominan \bi{iguales} si:
  \begin{align*}
    u_1=v_1, u_2=v_2,\vdots ,u_n=v_n
  \end{align*}
  La \bi{suma} $\mathbf{u}+\mathbf{v}$ se define por:
  \begin{align*}
    \mathbf{u}+\mathbf{v}=(u_1+v_1,u_2+v_2,\ldots,u_n+v_n)
  \end{align*}
  Si $k$ es cualquier escalar, entonces el \bi{múltiplo escalar} $k\mathbf{u}$
  se define por:
  \begin{align*}
    k\mathbf{u}=(ku_1,ku_2,\ldots,ku_n)
  \end{align*}
\end{concept}

Las operaciones de adición y multiplicación escalar en esta definición se
denominan \bi{operaciones normales} sobre $R^n$.

El \bi{vector cero} en $R^n$ se denota $\mathbf{0}$ y se define como el
vector:
\begin{align*}
  \mathbf{0}=(0,0,\ldots,0)
\end{align*}

Si $\mathbf{u}=(u_1,u_2,\ldots,u_n)$ es cualquier vector en $R^n$, entonces el
\bi{negativo} (o \bi{inverso aditivo}) de $\mathbf{u}$ se denota por
$-\mathbf{u}$ y se define por:
\begin{align*}
  -\mathbf{u}=(-u_1,-u_2,\ldots,-u_n)
\end{align*}

La \bi{diferencia} de vectores en $R^n$ se define por:
\begin{align*}
  \mathbf{v}-\mathbf{u}=\mathbf{v}+(-\mathbf{u})=(v_1-u_1,v_2-u_2,\ldots,v_n-u_n)
\end{align*}

%------------------------------------------
\subsubsection{Propiedades de las operaciones vectoriales en el espacio
$n$ dimensional}

\begin{theorem}
  Si $\mathbf{u}=(u_1,u_2,\ldots,u_n)$, $\mathbf{v}=(v_1,v_2,\ldots,v_n)$ y
  $\mathbf{w}=(w_1,w_2,\ldots,w_n)$ son vectores en $R^n$ y $k$ y $l$ son
  escalares entonces:
  \begin{enumerate}[(a)]
    \item $\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}$
    \item
      $\mathbf{u}+(\mathbf{v}+\mathbf{w})=(\mathbf{v}+\mathbf{u})+\mathbf{w}$
    \item $\mathbf{u}+\mathbf{0}=\mathbf{0}+\mathbf{u}=\mathbf{u}$
    \item $\mathbf{u}+(-\mathbf{u})=\mathbf{0}$
    \item $k(l\mathbf{u})=(kl)\mathbf{u}$
    \item $k(\mathbf{u}+\mathbf{v})=k\mathbf{u}+k\mathbf{v}$
    \item $(k+l)\mathbf{u}=k\mathbf{u}+l\mathbf{u}$
    \item $1\mathbf{u}=\mathbf{u}$
  \end{enumerate}
  \label{theo:prop_esp_vec}
\end{theorem}

\begin{concept}
  Si $\mathbf{u}=(u_1,u_2,\ldots,u_n)$ y $\mathbf{v}=(v_1,v_2,\ldots,v_n)$ son
  vectores cualesquiera en $R^n$, entonces el \bi{producto interior
  euclidiano} $\mathbf{u}\cdot\mathbf{v}$ se define por:
  \begin{align*}
    \mathbf{u}\cdot\mathbf{v}=\sum_{i=1}^n u_iv_i
  \end{align*}
\end{concept}

Como muchos de los conceptos conocidos de los espacios bidimensional y
tridimensional existen en el espacio $n$ dimensional, es común referirse a
$R^n$, con las operaciones de adición, multiplicación escalar y producto
interior euclidiano que se han definido aquí, como \bi{espacio euclidiano
$n$ dimensional}.

El siguiente teorema enumera las cuatro propiedades aritméticas más
importantes del producto interior euclidiano:
\begin{theorem}
  Si $\mathbf{u}$, $\mathbf{v}$ y $\mathbf{w}$ son vectores en $R^n$ y
  $k$ es cualquier escalar, entonces:
  \begin{enumerate}[(a)]
    \item $\mathbf{u}\cdot\mathbf{v}=\mathbf{v}\cdot\mathbf{u}$
    \item
      $(\mathbf{u}+\mathbf{v})\cdot\mathbf{w}=\mathbf{u}\cdot\mathbf{w}+\mathbf{v}\cdot\mathbf{w}$
    \item $(k\mathbf{u})\cdot\mathbf{v}=k(\mathbf{u}\cdot\mathbf{v})$
    \item $\mathbf{u}\cdot\mathbf{u}\ge 0$. Además,
      $\mathbf{u}\cdot\mathbf{u}=0$ si y sólo si $\mathbf{u}=\mathbf{0}$
  \end{enumerate}
  \label{theo:prop_prod_int}
\end{theorem}

%------------------------------------------
\subsubsection{Norma y distancia en el espacio euclidiano $n$ dimensional}

La \bi{norma euclidiana} (o \bi{longitud euclidiana}) de un vector
$\mathbf{u}=(u_1,u_2,\ldots,u_n)$ en $R^n$ se define como:
\begin{align*}
  ||\mathbf{u}||=\sqrt{\mathbf{u}\cdot\mathbf{u}}=\sqrt{\sum_{i=1}^nu_i^2}
\end{align*}

De manera semejante, la \bi{distancia euclidiana} entre los puntos
$\mathbf{u}=(u_1,u_2,\ldots,u_n)$ y $\mathbf{v}=(v_1,v_2,\ldots,v_n)$ en
$R^n$ se define por:
\begin{align*}
  d(\mathbf{u},\mathbf{v})=||\mathbf{u}-\mathbf{v}||=\sqrt{\sum_{i=1}^n(u_i-v_i)^2}
\end{align*}

\begin{theorem}[Desigualdad de Cauchy-Schwarz en $R^n$]
  Si $\mathbf{u}=(u_1,u_2,\ldots,u_n)$ y $\mathbf{v}=(v_1,v_2,\ldots,v_n)$ son
  vectores en $R^n$ entonces:
  \begin{align*}
    |\mathbf{u}\cdot\mathbf{v}|\le||\mathbf{u}||\;||\mathbf{v}||
  \end{align*}
  \label{theo:Cauchy}
\end{theorem}

\begin{theorem}
  Si $\mathbf{u}$ y $\mathbf{v}$ son vectores en $R^n$ y $k$ cualquier
  escalar, entonces:
  \begin{enumerate}[(a)]
    \item $||\mathbf{u}||\ge0$
    \item $||\mathbf{u}||=0$ si y sólo si $\mathbf{u}=\mathbf{0}$
    \item $||k\mathbf{u}||=|k|\;||\mathbf{u}||$
    \item $||\mathbf{u}+\mathbf{v}||\le||\mathbf{u}||+||\mathbf{v}||$
      (Desigualdad del triángulo)
  \end{enumerate}
  \label{theo:prop_long_dist}
\end{theorem}

\demo Se demostrará el inciso \emph{(d)}:
\begin{align*}
  ||\mathbf{u}+\mathbf{v}||^2&=(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}+\mathbf{v})=(\mathbf{u}\cdot\mathbf{u})+2(\mathbf{u}\cdot\mathbf{v})+(\mathbf{v}\cdot\mathbf{v})\\
  &=||\mathbf{u}||^2+2(\mathbf{u}\cdot\mathbf{v})+||\mathbf{v}||^2 \\
  &\le ||\mathbf{u}||^2+2|\mathbf{u}\cdot\mathbf{v}|+||\mathbf{v}||^2 \qquad
  \mbox{\emph{Propiedad del valor absoluto}}\\
  &\le ||\mathbf{u}||^2+2||\mathbf{u}||\;||\mathbf{v}||+||\mathbf{v}||^2 \quad
  \mbox{\emph{Propiedad de Cauchy-Schwarz}}\\
  &=\left( ||\mathbf{u}||+||\mathbf{v}|| \right)^2
\end{align*}
Extrayendo la raíz cuadrada a ambos miembros se concluye que:
\begin{align*}
  ||\mathbf{u}+\mathbf{v}||\le||\mathbf{u}||+||\mathbf{v}||
\end{align*}

\begin{theorem}
  Si $\mathbf{u}$ y $\mathbf{v}$ son vectores en $R^n$ con producto interior
  euclidiano, entonces:
  \begin{align}
    \mathbf{u}\cdot\mathbf{v}=\tfrac{1}{4}||\mathbf{u}+\mathbf{v}||-\tfrac{1}{4}||\mathbf{u}-\mathbf{v}||
    \label{eqn:alt_prod_int}
  \end{align}
  \label{theo:prop_long_dist}
\end{theorem}

\demo
\begin{align*}
  ||\mathbf{u}+\mathbf{v}||^2&=(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}+\mathbf{v})=||\mathbf{u}||^2+2(\mathbf{u}\cdot\mathbf{v})+||\mathbf{v}||^2
  \\
  ||\mathbf{u}-\mathbf{v}||^2&=(\mathbf{u}-\mathbf{v})\cdot(\mathbf{u}-\mathbf{v})=||\mathbf{u}||^2-2(\mathbf{u}\cdot\mathbf{v})+||\mathbf{v}||^2
\end{align*}
por álgebra simple se puede llegar a \eqref{eqn:alt_prod_int}.

%------------------------------------------
\subsubsection{Ortogonalidad}

\begin{concept}
  Dos vectores $\mathbf{u}$ y $\mathbf{v}$ en $R^n$ se denominan ortogonales
  si $\mathbf{u}\cdot\mathbf{v}=0$
\end{concept}

\begin{theorem}[Teorema de Pitágoras para $R^n$]
  Si $\mathbf{u}$ y $\mathbf{v}$ son vectores ortogonales en $R^n$ con
  producto interior euclidiano, entonces:
  \begin{align*}
    ||\mathbf{u}+\mathbf{v}||^2=||\mathbf{u}||^2+||\mathbf{v}||^2
  \end{align*}
  \label{theo:pitagoras}
\end{theorem}

%------------------------------------------
\subsubsection{Notación matricial}

Un vector $\mathbf{u}=(u_1,u_2,\ldots,u_n)$ en $R^n$ se puede escribir en
notación matricial como matriz fila o matriz columna:
\begin{align*}
  \mathbf{u}=
  \begin{bmatrix}
    u_1 \\
    u_2 \\
    \vdots \\
    u_n
  \end{bmatrix} \qquad \mbox{o} \qquad
  \mathbf{u}=
  \begin{bmatrix}
    u_1 & u_2 & \cdots & u_n
  \end{bmatrix}
\end{align*}

%------------------------------------------
\subsubsection{Fórmula matricial para el producto punto}

Si los vectores se escriben como matrices columnas:
\begin{align*}
  \mathbf{u}=
  \begin{bmatrix}
    u_1 \\
    u_2 \\
    \vdots \\
    u_n
  \end{bmatrix} \qquad \mbox{y} \qquad
  \mathbf{v}=
  \begin{bmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
  \end{bmatrix}
\end{align*}
y en las matrices $1\times 1$ se omiten los corchetes, entonces se deduce que:
\begin{align*}
  \mathbf{v}^T\mathbf{u}=
  \begin{bmatrix}
    v_1 & v_2 & \cdots & v_n
  \end{bmatrix}
  \begin{bmatrix}
    u_1 \\
    u_2 \\
    \vdots \\
    u_n
  \end{bmatrix}
  =[u_1v_1+u_2v_2+\ldots+u_nv_n]
  =[\mathbf{u}\cdot\mathbf{v}]=\mathbf{u}\cdot\mathbf{v}
\end{align*}
Así que para vectores expresados como matrices columna se tiene la siguiente
fórmula para el producto interior euclidiano:
\begin{align*}
  \mathbf{v}^T\mathbf{u}=\mathbf{u}\cdot\mathbf{v}
\end{align*}

%------------------------------------------
\subsubsection{Un producto punto considerado como multiplicación matricial}

El $ij$-ésimo elemento de la matriz resultante del producto de dos matrices
$A_{m\times r}B_{r\times n}$ es:
\begin{align*}
  \left[ AB \right]_{ij}=\sum_{k=1}^r a_{ik}b_{kj}
\end{align*}
que es el producto punto del $i$-ésimo vector fila de $A$
\begin{align*}
  \begin{bmatrix}
    a_{i1} & a_{i2} & \cdots & a_{ir}
  \end{bmatrix}
\end{align*}
y el $j$-ésimo vector columna de $B$
\begin{align*}
  \begin{bmatrix}
    b_{1j} \\
    b_{2j} \\
    \vdots \\
    b_{rj}
  \end{bmatrix}
\end{align*}
Por lo tanto, si los vectores fila de $A$ son $\mathbf{r}_1, \mathbf{r}_2,
\ldots, r_n$ y los vectores columna de $B$ son $\mathbf{c}_1, \mathbf{c}_2,
\ldots, c_n$, entonces el producto matricial $AB$ se puede expresar como:
\begin{align*}
  AB=
  \begin{bmatrix}
    \mathbf{r}_1\cdot\mathbf{c}_1 & \mathbf{r}_1\cdot\mathbf{c}_2 & \cdots &
    \mathbf{r}_1\cdot\mathbf{c}_n \\
    \mathbf{r}_2\cdot\mathbf{c}_1 & \mathbf{r}_2\cdot\mathbf{c}_2 & \cdots &
    \mathbf{r}_2\cdot\mathbf{c}_n \\
    \vdots  & \vdots  &    &  \vdots \\
    \mathbf{r}_m\cdot\mathbf{c}_1 & \mathbf{r}_m\cdot\mathbf{c}_2 & \cdots &
    \mathbf{r}_m\cdot\mathbf{c}_n
  \end{bmatrix}
\end{align*}

%------------------------------------------
%------------------------------------------
\subsection{Transformaciones lineales de $R^n$ a $R^m$}

\subsubsection{Funciones de $R^n$ a $R^m$}

Si el dominio de una función $f$ es $R^n$ y la imagen es $R^m$ ($m$ y
$n$ quizá iguales), entonces $f$ se denomina \bi{transformación} de $R^n$ a
$R^m$, y se dice que $f$ \bi{mapea} (aplica o transforma) $R^n$ en $R^m$. Este
hecho se denota escribiendo $f:R^n\rightarrow R^m$. Para el caso especial en
el que $n=m$, la transformación $f:R^n\rightarrow R^n$ se denomina
\bi{operador} sobre $R^n$.

Para ilustrar una forma importante en que pueden surgir las transformaciones,
suponer que $f_1, f_2, \ldots, f_m$ son funciones con valores reales de
$n$ variables reales, por ejemplo:
\begin{align}
  \begin{matrix}
    w_1&=&f_1(x_1,x_2,\ldots,x_n)\\
    w_2&=&f_2(x_1,x_2,\ldots,x_n)\\
    \vdots& &\vdots \\
    w_m&=&f_m(x_1,x_2,\ldots,x_n)
  \end{matrix}
  \label{eqn:transf_rnrm}
\end{align}
Estas $m$ ecuaciones asignan un punto único $(w_1,w_2,\ldots,w_m)$ en
$R^m$ a cada punto $(x_1,x_2,\ldots,x_n)$ en $R^n$ y por tanto, definen una
transformación de $R^n$ a $R^m$. Si esta transformación se denota por
$T$, entonces $T:R^n\rightarrow R^m$ y
\begin{align*}
  T(x_1,x_2,\ldots,x_n)=(w_1,w_2,\ldots,w_m)
\end{align*}
 
%------------------------------------------
\subsubsection{Transformaciones lineales de $R^n$ a $R^m$}

En el caso especial en que las ecuaciones \eqref{eqn:transf_rnrm} son
lineales, la transformación $T:R^n\rightarrow R^m$ definida por esas
ecuaciones se denomina \bi{transformación lineal} (u \bi{operador lineal} si
$n=m$). Así, una transformación lineal $T:R^n\rightarrow R^m$ está definida
por ecuaciones de la forma:
\begin{align*}
  \begin{matrix}
    w_1   &=& a_{11}x_1 &+& a_{12}x_2 &+& \cdots &+& a_{1n}x_n \\
    w_2   &=& a_{21}x_1 &+& a_{22}x_2 &+& \cdots &+& a_{2n}x_n \\
    \vdots& & \vdots    & &\vdots     & &        & &\vdots     \\
    w_m   &=& a_{m1}x_1 &+& a_{m2}x_2 &+& \cdots &+& a_{mn}x_n 
  \end{matrix}
\end{align*}
o bien, en notación matricial:
\begin{align*}
  \begin{bmatrix}
    w_1    \\
    w_2    \\
    \vdots \\
    w_m
  \end{bmatrix} =
  \begin{bmatrix}
    a_{11}& a_{12}& \cdots& a_{1n}\\
    a_{21}& a_{22}& \cdots& a_{2n}\\
    \vdots&\vdots &       &\vdots \\
    a_{m1}& a_{m2}& \cdots& a_{mn}
  \end{bmatrix}
  \begin{bmatrix}
    x_1    \\
    x_2    \\
    \vdots \\
    x_n
  \end{bmatrix}
\end{align*}
o más brevemente,
\begin{align*}
  \mathbf{w}=A\mathbf{x}
\end{align*}
La matriz $A=[a_{ij}]$ se denomina \bi{matriz estándar} de la transformación
lineal $T$ y $T$ se denomina \bi{multiplicación por A}.

%------------------------------------------
\subsubsection{Algunos comentarios sobre la notación}

Si $T:R^n\rightarrow R^m$ es una multiplicación por $A$, y si es importante
recalcar que $A$ es la matriz estándar para $T$, entonces la transformación
lineal $T:R^n\rightarrow R^m$ se denota por $T_A:R^n\rightarrow R^m$. Así:
\begin{align*}
  T_A(\mathbf{x})=A\mathbf{x}
\end{align*}
En esta ecuación se sobrentiende que el vector $\mathbf{x}$ en $R^n$ se
expresa como una matriz columna.

Otra notación utilizada para la matriz estándar es:
\begin{align*}
  [T_A]=A
\end{align*}

%------------------------------------------
\subsubsection{Composiciones de transformaciones lineales}

Si $T_A:R^n\rightarrow R^k$ y $T_B:R^k\rightarrow R^m$ son transformaciones
lineales, entonces para todo $\mathbf{x}$ en $R^n$ primero se puede calcular
$T_A(\mathbf{x})$, que es un vector en $R^k$, y luego calcular $T_B\left(
T_A(\mathbf{x}) \right)$, que es un vector en $R^m$. Así, la aplicación de
$T_A$ seguida de $T_B$ produce una transformación de $R^n$ a $R^m$. Esta
transformación se denomina \bi{composición de $T_B$ con $T_A$} y se denota por
$T_B\circ T_A$ (y se lee como ``$T_A$ seguida de $T_B$''). Así:
\begin{align*}
  \left( T_B\circ T_A \right)(\mathbf{x})=T_B\left( T_A(\mathbf{x}) \right)
\end{align*}
La composición $T_B\circ T_A$ es lineal, ya que:
\begin{align}
  \left( T_B\circ T_A \right)(\mathbf{x})=T_B\left( T_A(\mathbf{x})
  \right)=B(A\mathbf{x})=(BA)\mathbf{x}
  \label{eqn:comp_trans_lin}
\end{align}
De modo que $T_B\circ T_A$ es la multiplicación por $BA$, que es una
transformación lineal. La fórmula \eqref{eqn:comp_trans_lin} también establece
que la matriz estándar para $T_B\circ T_A$ es $BA$. Este hecho se expresa con
la fórmula:
\begin{align}
  T_B\circ T_A = T_{BA}
  \label{eqn:comp_trans_prod_matr}
\end{align}

\obse La fórmula \eqref{eqn:comp_trans_prod_matr} encierra la siguiente idea
importante:
\begin{concept}[i]
  La multiplicación de matrices es equivalente a componer las transformaciones
  lineales correspondientes en orden de derecha a izquierda de los factores.
\end{concept}

La fórmula \eqref{eqn:comp_trans_prod_matr} se puede escribir de otra manera:
\begin{align*}
  [T_2 \circ T_1]=[T_2][T_1]
\end{align*}


%------------------------------------------
%------------------------------------------
\subsection{Propiedades de las transformaciones lineales de $R^n$ a $R^m$}

%------------------------------------------
\subsubsection{Transformaciones lineales uno a uno}

Las transformaciones lineales que mapean vectores distintos en vectores
distintos revisten especial importancia.

\begin{concept}
  Se dice que una transformación $T:R^n\rightarrow R^m$ es \bi{biyectiva}
  (o \bi{uno a uno}) si $T$ mapea vectores distintos de $R^n$ en vectores
  distintos de $R^m$
\end{concept}

Sea $A$ una matriz $n\times n$, y sea $T_A:R^n\rightarrow R^m$ la
multiplicación por $A$. A continuación se analizarán las relaciones entre
invertibilidad de $A$ y las propiedades de $T_A$.

El siguiente teorema se aplica para los operadores lineales sobre $R^n$.
\begin{theorem}
  Si $A$ es una matriz $n\times n$ y $T_A:R^n\rightarrow R^n$ es la
  multiplicación por $A$, entonces las siguientes proposiciones son
  equivalentes:
  \begin{enumerate}[(a)]
    \item A es invertible.
    \item El recorrido de $T_A$ es $R^n$
    \item $T_A$ es uno a uno.
  \end{enumerate}
  \label{theo:equiv_oper_lin}
\end{theorem}
Estas afirmaciones se obtienen a partir del teorema
\ref{theo:resumen_matriz_det} al traducir en proposiciones correspondientes
respecto al operador lineal $T_A$:
\begin{itemize}
  \item Para todo vector $\mathbf{w}$ en $R^n$, existe algún vector
    $\mathbf{x}$ en $R^n$ tal que $T_A(\mathbf{x})=\mathbf{w}$. Expresado de
    otra forma, el recorrido de $T_A$ es todo $R^n$.
  \item Para todo vector $\mathbf{w}$ en el recorrido de $T_A$, existe
    exactamente un vector $\mathbf{x}$ en $R^n$ tal que
    $T_A(\mathbf{x})=\mathbf{w}$. Planteado de otra forma, $T_A$ es uno a uno.
\end{itemize}

%------------------------------------------
\subsubsection{Inversa de un operador lineal uno a uno}

Si $T_A:R^n\rightarrow R^n$ es un operador lineal uno a uno, entonces por el
teorema \ref{theo:equiv_oper_lin}, la matriz $A$ es invertible. Así
$T_{A^{-1}}:R^n\rightarrow R^n$ por sí mismo es un operador lineal y se
denomina \bi{inverso de $T_A$}. Los operadores lineales $T_A$ y $T_{A^{-1}}$
se cancelan entre sí:
\begin{align*}
  T_A(T_{A^{-1}}(\mathbf{x}))=AA^{-1}\mathbf{x}=I\mathbf{x}=\mathbf{x} \\
  T_{A^{-1}}(T_A(\mathbf{x}))=A^{-1}A\mathbf{x}=I\mathbf{x}=\mathbf{x}
\end{align*}

%------------------------------------------
\subsubsection{Propiedades de la linealidad}

\begin{theorem}
  Una transformación $T_:R^n\rightarrow R^m$ es lineal si y sólo si las
  siguientes relaciones se cumplen para todos los vectores $\mathbf{u}$ y
  $\mathbf{v}$ en $R^n$ y cualquier escalar $c$.
  \begin{enumerate}[(a)]
    \item $T(\mathbf{u}+\mathbf{v})=T(\mathbf{u})+T(\mathbf{v})$
    \item $T(c\mathbf{u})=cT(\mathbf{u})$
  \end{enumerate}
  \label{theo:transf_lineal}
\end{theorem}

\demo Se puede demostrar de dos formas:
Primero se supone que $T$ es una transformación lineal, y se hace que $A$ sea
la matriz estándar para $T$. Por las propiedades aritméticas básicas de las
matrices se concluye que:
\end{document}
